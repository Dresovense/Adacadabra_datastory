---
layout: page
title: The War of The Learned
subtitle: Data-driven Linguistic Quality Analysis based on Reddit data
---

<!-- ======================================================= -->
<!-- GLOBAL STYLING SYSTEM                                   -->
<!-- Change --theme-color here to update the whole page      -->
<!-- ======================================================= -->
<style>
    :root {
        /* CORE THEME COLORS */
        --theme-color: #11224d;       /* The main Deep Blue */
        --theme-light: #e6edff;       /* Very light blue for backgrounds */
        --theme-accent: #f57f17;      /* Orange for alerts/todos */
        
        /* TEXT & LAYOUT */
        --text-main: #333333;
        --text-muted: #555555;
        --border-radius: 8px;
        --spacing-section: 80px;
        --spacing-item: 30px;
    }

    /* --- LAYOUT UTILITIES --- */
    .ds-section {
        margin-bottom: var(--spacing-section);
        padding-top: 30px;
        border-top: 1px solid #eee;
        color: var(--text-main);
        line-height: 1.6;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
        text-align: justify;
    }
    .ds-section:first-of-type { border-top: none; padding-top: 0; }

    .ds-grid-2 {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: var(--spacing-item);
        margin: var(--spacing-item) 0;
    }

    .ds-flex-row {
        display: flex;
        gap: var(--spacing-item);
        align-items: flex-start;
        margin: var(--spacing-item) 0;
    }

    /* --- VISUAL COMPONENTS --- */
    
    /* 1. Plot Frames */
    .ds-frame {
        width: 100%;
        /* border: 2px solid var(--theme-color);  */
        border-radius: var(--border-radius);
        background: white; 
        overflow: hidden;
        margin-bottom: 10px;
        box-shadow: 0 4px 10px rgba(0,0,0,0.1);
        display: flex;
        align-items: center;
        justify-content: center;
        box-sizing: border-box;
        position: relative;
    }
    
    .ds-frame iframe {
        width: 100%;
        height: 100%;
        border: none;
        display: block;
    }

    .ds-caption {
        font-size: 0.9em;
        font-style: italic;
        color: var(--text-muted);
        text-align: center;
        margin-top: 8px;
        display: block;
    }

    /* 2. Images & Reddit Posts */
    .ds-img-responsive {
        max-width: 100%;
        height: auto;
        object-fit: contain;
    }
    
    .ds-reddit-post {
        display: block;
        margin: var(--spacing-item) auto var(--spacing-item) auto;
        max-width: 60%;
        border-radius: var(--border-radius);
        box-shadow: 0 10px 30px rgba(0,0,0,0.15); 
        border: 1px solid rgba(0,0,0,0.05);
    }

    /* 3. Quotes */
    .ds-blockquote {
        border-left: 6px solid var(--theme-color);
        background-color: #f8f9fa;
        padding: 20px 25px;
        margin: 30px 0;
        font-style: italic;
        color: #444;
        border-radius: var(--border-radius); 
        box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        font-size: 1.05em;
        line-height: 1.7;
    }

    /* 4. Lists, Clusters & Tables */
    .ds-list-box {
        background: #f8f9fa;
        padding: 20px;
        border-radius: var(--border-radius);
        border-left: 5px solid var(--theme-color);
        height: fit-content;
        box-sizing: border-box;
    }
    
    .ds-list-clean ul { list-style: none; padding: 0; margin: 0; }
    .ds-list-clean li { padding: 6px 0; border-bottom: 1px solid #e0e0e0; font-size: 0.95em; }
    .ds-list-clean li:last-child { border-bottom: none; }
    .ds-list-clean strong { color: var(--theme-color); display: inline-block; min-width: 30px; }

    /* Data Tables inside list boxes */
    .ds-table { width: 100%; border-collapse: collapse; font-size: 0.85em; }
    .ds-table th { color: #333; padding: 8px; text-align: left; font-weight: bold; border: none; }
    .ds-table td { padding: 8px; border: none; }
    .ds-table tr:last-child td { border: none; }
    
    .ds-table-header { 
        border: none;
        background: #e9ecef; 
        color: #555; 
        font-weight: bold; 
        text-align: center; 
        padding: 6px; 
        font-size: 0.85em; 
        letter-spacing: 1px; 
        text-transform: uppercase; 
    }

    .val-high { color: var(--theme-color); font-weight: 700; border: 0 1px 0 1px; }
    .val-low { color: #d32f2f; font-weight: 700; border: none; }

    /* 5. Abstracts & Todos */
    .ds-abstract {
        background-color: var(--theme-light);
        border-left: 6px solid var(--theme-color);
        padding: 20px;
        border-radius: var(--border-radius); 
        margin: var(--spacing-item) 0 var(--spacing-item) 0;
        color: var(--text-main);
    }
    
    .ds-todo {
        background-color: #fff9c4;
        border: 1px dashed var(--theme-accent);
        color: #444;
        padding: 15px;
        margin: 20px 0;
        font-family: monospace;
        font-size: 0.9em;
    }

    /* 6. Metric Accordion Styling */
    .accordion-wrapper {
        border-left: 6px solid #0b1e47; 
        background-color: #f8f9fa;
        padding: 20px;
        margin: 25px 0;
        border-radius: var(--border-radius);
        box-shadow: 0 4px 6px rgba(0,0,0,0.05);
    }

    .metric-item {
        background-color: white;
        border: 1px solid #e0e0e0;
        border-radius: var(--border-radius);
        margin-bottom: 10px;
        overflow: hidden;
        transition: box-shadow 0.2s;
    }
    .metric-item:hover { box-shadow: 0 2px 5px rgba(0,0,0,0.05); }

    .metric-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 15px;
        cursor: pointer;
        background-color: #fff;
    }
    .metric-header:hover { background-color: #f9f9f9; }

    .metric-name { font-weight: bold; font-size: 1.1em; color: var(--theme-color); }
    .metric-arrow { color: var(--theme-color); transition: transform 0.3s; }
    .metric-item.active .metric-arrow { transform: rotate(180deg); }

    .metric-content {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.3s ease-out;
        padding: 0 15px;
    }
    .metric-inner { padding: 15px 0; font-size: 0.95em; color: #333; border-top: 1px solid #eee; }
    
    .metric-formula {
        background-color: #f1f3f5;
        padding: 4px 8px;
        border-radius: 4px;
        font-family: monospace;
        font-size: 0.9em;
        color: #d63384;
        display: inline-block;
        margin-bottom: 8px;
    }
    
    /* Container for the stack */
    .insight-stack {
        display: flex;
        flex-direction: column;
        gap: 15px; /* Space between boxes */
        margin: 40px 0;
    }

    /* Individual Card Style */
    .insight-card {
        background: white;
        border-radius: var(--border-radius);
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        /* border: 1px solid #e0e0e0; */
        border: none;
        overflow: hidden; /* Hides content when closed */
        transition: all 0.3s ease;
    }

    /* The Clickable Header */
    .insight-header {
        padding: 20px 25px;
        background: white;
        cursor: pointer;
        display: flex;
        justify-content: space-between;
        align-items: center;
        border-left: 6px solid var(--theme-color); /* The colored strip */
        transition: background-color 0.2s;
    }

    .insight-header:hover {
        background-color: #f8f9fa;
    }

    .insight-title {
        font-weight: 700;
        font-size: 1.1em;
        color: var(--text-main);
        margin: 0;
    }

    .insight-icon {
        font-size: 1.2em;
        color: var(--theme-color);
        transition: transform 0.3s ease;
    }

    /* The Unfolding Content */
    .insight-body {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.4s ease-out, opacity 0.3s ease;
        opacity: 0;
        background-color: #fafbfc;
        border-top: 1px solid transparent;
    }

    .insight-content-inner {
        padding: 25px;
        color: var(--text-muted);
        font-size: 0.95em;
    }

    /* ACTIVE STATE (When open) */
    .insight-card.active {
        box-shadow: 0 8px 20px rgba(0,0,0,0.1);
        border-color: var(--theme-color);
    }
    
    .insight-card.active .insight-header {
        background-color: var(--theme-light);
    }

    .insight-card.active .insight-icon {
        transform: rotate(180deg); /* Flip arrow */
    }

    .insight-card.active .insight-body {
        opacity: 1;
        border-top-color: #eee;
    }

    /* Custom borders for different topics to add variety */
    .insight-card:nth-child(1) .insight-header { border-left-color: #2196F3; } /* Blue for Tech */
    .insight-card:nth-child(2) .insight-header { border-left-color: #673AB7; } /* Purple for Conspiracy */
    .insight-card:nth-child(3) .insight-header { border-left-color: #4CAF50; } /* Green for Sports */
    .insight-card:nth-child(4) .insight-header { border-left-color: #F44336; } /* Red for Outliers */

    /* RESPONSIVE */
    @media (max-width: 768px) {
        .ds-grid-2, .ds-flex-row { grid-template-columns: 1fr; flex-direction: column; }
        .ds-reddit-post, .ds-img-main { max-width: 100%; }
    }
</style>

<!-- ======================================================= -->
<!-- CONTENT START                                           -->
<!-- ======================================================= -->

<img src="assets/reddit_posts/post_1.png" class="ds-reddit-post">

<!-- SECTION: INTRO -->
<div class="ds-section">
    <!-- Abstract Box -->
    <div class="ds-abstract">
        <strong>TLDR;</strong> Is the internet really getting "dumber", or is it just getting more diverse? We analyzed millions of Reddit posts using 300-dimensional embeddings and linguistic algorithms to map the "Digital Dialects" of the internet. We found that while some communities thrive on simplicity, high-complexity language is alive and well‚Äîoften in the most unexpected places.
    </div>

    <p>
        It's not the first time I've seen such sentiments around here. Actually,
        I find it super interesting, as such discussions about "linguistic quality" are not recent.
        In fact you can find them pretty much at any point since we've begun to standardise our languages. 
        Still, with the rise of social media and instant messaging, such perceptions have been on the rise
        and it could be interesting to know if they are based on actual data. And
        if so, what motivates the difference of language quality between users.
    </p>

    <div class="ds-todo">
        [Optional: We can formalize Research Questions (RQ1, RQ2) here if we want a more academic tone, or leave it narrative.]
    </div>
</div>

<!-- SECTION: DATASET -->
<div class="ds-section">
    <h1>1. Dataset</h1>
    
    <h2>Hyperlinks</h2>
    <p>
        Our primary dataset consists of the "Subreddit Hyperlinks Network." This is a massive collection 
        of posts where one subreddit links to another. It provides us with the raw text body of the posts, 
        timestamps, and sentiment labels, allowing us to analyze not just what is said, but the context in which it is said.
    </p>

    <h2>Embeddings</h2>
    
    <h3>Introduction</h3>
    <p>
        Along with the initial subreddit hyperlinks dataset, we used another dataset: 
        the embedding vectors of subreddits (available on the SNAP website).
    </p>
    <p>
        These embeddings are high-dimensional vectors (in our case, 300 dimensions). 
        They are designed to represent similarities between data points in a complex space. 
        Specifically, each of the ~50,000 subreddits is assigned a 300-dimensional vector. 
        These vectors indicate similarity based on user behavior: if many users post in 
        the same group of subreddits, those subreddits will be closer to each other in this
        300-dimensional space.
    </p>
    <p>
        These embeddings are very useful for our research on linguistic quality and our goal to determine 
        'who speaks the best.' They allow us to identify clusters of subreddits where users share similar 
        interests, effectively grouping subreddits into distinct communities.
    </p>
    
    <h3>Weakness</h3>
    <p>
        While these embeddings are powerful and perform well, the dataset faces challenges due to the
        nature of Reddit communities and the high number of data.    
    </p>
    
    <div class="ds-blockquote">
        "One might assume that a dataset of 50,000 subreddits is relatively small. However, each 
        entry is a 300-dimensional vector... Calculating the similarity between every possible pair 
        of subreddits is a computationally intensive task...
        To overcome this, we implemented a batch processing approach."
    </div>

    <p>
        Indeed communities are not perfectly separated into 'clean' clusters where we can easily say: 
        'These users are only interested in politics.' Most users have diverse interests and post in a wide
        variety of subreddits. This creates a significant amount of overlap between vectors. Consequently, 
        some subreddits appear very close to many others in the 300-dimensional space, which can create 'noisy' 
        links and blur the boundaries between different communities.
    </p>

    <h3>Visualizing the Embedding Weaknesses</h3> 
    <p>
        The following graphs illustrate the "fuzzy" nature of these communities.
        On <b>Figure 1</b>, a 2D projection (using dimensionality reduction) of the 50,000 subreddits shows the vectors 
            forming a large, dense circle with significant overlap. This visualization confirms that many communities
            are not clearly separated.
        On <b>Figure 2</b>, the graph displays the distribution of close neighbors (subreddits with a cosine similarity > 0.8).
            Cosine similarity measures the orientation of two vectors in the 300-dimensional space, ranging from -1 (opposite)
            to 1 (identical). The distribution reveals a non-negligible number of subreddits that have more than 10,000 neighbors 
            with over 80% similarity.
    </p>

    <!-- PLOT 1 -->
    <div style="margin-bottom: 40px;">
        <div class="ds-frame" style="height: 600px;">
            <iframe src="assets/plots/embeddings_and_clustering/reddit_map.html"></iframe>
        </div>
        <span class="ds-caption">Figure 1: 2D Projection of Subreddits (The "Blob" of Reddit)</span>
    </div>

    <!-- PLOT 2 -->
    <div style="margin-bottom: 40px;">
        <div class="ds-frame" style="height: 500px;">
            <img src="assets/plots/embeddings_and_clustering/distribution_of_closed_neigbors_embeddings.png" class="ds-img-responsive">
        </div>
        <span class="ds-caption">Figure 2: Distribution of Close Neighbors</span>
    </div>
</div>

<!-- SECTION: DEFINING LANGUAGE -->
<div class="ds-section">
    <h1>2. What is language quality?</h1>
    <div class="ds-abstract">
        <strong>TLDR;</strong>  
        The first thing we need to know when trying to understand language quality
        on reddit, is to actually define what language quality <em>is</em>. We could 
        define language quality of a text as "<b>the similarity between a text and
        a shared construction of the "perfect" language</b>. [do you agree? should i remove it?]
        However, we cannot apply directly such a definition to our data, we need to 
        find a way to operationalise such a concept. Thus, I decided to take an
        approach that combined both what our data provided and what different measures
        suggested that this "shared construction" is, arriving at the following metrics.
    </div>

    <!-- START: Metric Accordion -->
    <div class="accordion-wrapper">
        <p style="margin-top:0;">To calculate the <b>Linguistic Quality Index (LQI)</b>, we aggregated four robust dimensions. Click below to see the exact formulas derived from our data processing.</p>
        
        <div class="metric-list">
            
            <!-- Metric 1 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Lexical Richness (Herdan's C)</span><span class="metric-arrow">‚ñº</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">log(Unique Words) / log(Total Words)</div>
                        <p>Standard Type-Token Ratio is biased against long texts (the longer you write, the more you repeat common words). We used Herdan's C (Log-TTR) to ensure fair comparison between short comments and long rants.</p>
                    </div>
                </div>
            </div>

            <!-- Metric 2 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Structural Complexity</span><span class="metric-arrow">‚ñº</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">(0.5 * Avg Word Len) + log(Avg Sentence Len)</div>
                        <p>Simple sentence length is noisy (a 50-word list of groceries is not "complex"). We created a composite score that rewards using longer, more complex words <i>within</i> structurally longer sentences.</p>
                    </div>
                </div>
            </div>

            <!-- Metric 3 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Formality Index</span><span class="metric-arrow">‚ñº</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">Articles - Pronouns - (2 * Swearing) - Uppercase</div>
                        <p>Based on Heylighen & Dewaele (2002). This distinguishes "Contextual" language (casual, chatty, subjective: "I think...") from "Formal" language (objective, noun-heavy: "The data suggests...").</p>
                    </div>
                </div>
            </div>

            <!-- Metric 4 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Cognitive Depth</span><span class="metric-arrow">‚ñº</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">Mean(LIWC_CogMech + LIWC_Insight + LIWC_Cause)</div>
                        <p>Aggregates words related to processing information (<i>think, know</i>) and causality (<i>because, hence</i>). It distinguishes between descriptive storytelling or emotional venting and analytical reasoning.</p>
                    </div>
                </div>
            </div>

        </div>
    </div>
    <!-- END: Metric Accordion -->

    <h3>Feature Analysis</h3>
    <div class="ds-frame" style="height: 530px;">
        <iframe src="assets/plots/linguistic_feature_histogram.html"></iframe>
    </div>
    
    <p>
        The histograms above show distinct characteristics. For example, <b>lexical_richness</b> shows a sharp spike at 1.0. This artifact represents very short posts (e.g., "Yes", "lol") where every word is unique. This confirms why simple ratios fail and why we need robust metrics. 
        Meanwhile, <b>cognitive_depth</b> often spikes at zero, indicating that a significant portion of Reddit communication is purely phatic or descriptive, lacking explicit reasoning words.
    </p>

    <div class="ds-grid-2">
        <div>
            <img src="assets/plots/feature_correlation_matrix.png" class="ds-img-responsive" style="border-radius:8px;">
        </div>
        <div>
            <h4>Orthogonality of Features</h4>
            <p>
                An essential check was to ensure our metrics weren't just measuring the same thing four times. 
                As the correlation matrix shows, our chosen features have low overlap. 
            </p>
            <p>
                Interestingly, <b>Lexical Richness</b> is negatively correlated with <b>Syntactic Complexity</b>. This makes sense: 
                complex academic texts often reuse specific terminology (low richness) within very long, complex sentences. 
                This confirms that each feature captures a unique dimension of the "Linguistic Profile."
            </p>
        </div>
    </div>
</div>

<!-- SECTION: SUBREDDIT QUALITY -->
<div class="ds-section">
    <h1>3. Linguistic quality within specific subreddits</h1>

    <div class="ds-abstract">
        <strong>TLDR;</strong> 
        Let's try to validate our instincts by checking up on the "best" and "worst" performing subreddits.
    </div>
    
    <!-- Interactive Plot -->
    <div class="ds-frame" style="height: 630px;">
        <iframe src="assets/plots/metric_distributions.html"></iframe>
    </div>
    <span class="ds-caption">
            Normalized distribution of linguistic metrics from individual subreddits. 
            Y-Axis represents percentage density to allow fair comparison between with differently sized subs with at least 500 posts. (single or double click on legend to isolate subreddits)
    </span>
    
    <div style="margin-top: 20px;">
        <p><b>Analysis of the Leaderboard:</b></p>
        <p>
            The distinction is stark. The top-performing subreddits are dominated by specific, 
            knowledge-heavy communities like <i>r/AskHistorians</i> or <i>r/explainlikeimfive</i>. These communities enforce strict 
            moderation policies that require detailed, cited responses, naturally inflating their Structural Complexity and Cognitive Depth scores.
        </p>
        <p>
            Conversely, the bottom-performing subreddits are populated by meme subreddits and rapid-fire gaming communities. 
            However, "low quality" here doesn't necessarily mean "stupid." It often reflects a different <i>function</i> 
            of language: efficient, high-context communication (memes, slang) that prioritizes speed and communities dependent text structures over formalities.
        </p>
    </div>
</div>

<!-- SECTION: CLUSTERING / COMMUNITY -->
<div class="ds-section">
    <h1>4. Linguistic variation within a topic/community</h1>
    <img src="assets/reddit_posts/comment_1.png" class="ds-reddit-post">

    <h2>Introduction</h2>

    <div class="ds-abstract">
        <strong>TLDR;</strong> 
        What is a community? Oxford Languages defines it as: 'a group of people 
        living in the same place or having a particular characteristic in common.'
        Applying this to Reddit, we can define a community as a group of users sharing a 
        common interest. In this section, our objective is to apply clustering algorithms
        to the subreddit embeddings. By doing so, we aim to group subreddits with similar 
        vector representations into distinct clusters, effectively mapping out digital communities 
        based on shared user interests.
    </div>

    <h2>Methodology</h2>
    <h3>Clustering Strategy and Challenges</h3>
    
    <p>
        The primary challenge in this analysis stems from <strong>"bridges"</strong>: 
        subreddits frequented by users from vastly different backgrounds. These bridges 
        exhibit a high number of neighbors, making them notoriously difficult to classify. 
        In standard clustering, these points either force the creation of massive, noisy clusters 
        (K-means) or are discarded entirely as outliers (HDBSCAN).
    </p>

    <div class="ds-list-box" style="margin-bottom: 20px;">
        <div class="ds-list-clean">
            <ul>
                <li>
                    <strong>Why K-means Failed:</strong> Our initial attempt using <strong>K-means</strong> proved inadequate. 
                    K-means inherently assumes spherical (circular) cluster shapes and requires 
                    <i>a-priori</i> knowledge of the exact number of clusters. Given the nature of 
                    Reddit, communities are not perfectly circular; they overlap extensively due to the diverse 
                    interests of users. Forcing these high-dimensional "clouds" into rigid spheres resulted 
                    in a poor representation of the social reality.
                </li>
                <li>
                    <strong>The Density Limitation of HDBSCAN:</strong> was a logical next step due to its ability to handle 
                    non-spherical shapes. However, it proved too selective for our dataset. 
                    While it identified high-density cores, the resulting clusters were too 
                    small, and a significant portion of the subreddits were labeled as outliers.
                </li>
            </ul>
        </div>
    </div>

    <h3>Refined Clustering Approach</h3>
    <p>
        At this stage, we realized our fundamental assumption was flawed: we were 
        looking for <strong>predefined topics</strong> (e.g., "politics", "video games"). 
        In reality, embeddings capture <strong>community behaviors</strong>. A cluster 
        might not represent a single subject, but rather a specific demographic of 
        users who share multiple interests.
    </p>

    <p>To refine the methodology, three key improvements were implemented:</p>
    <ol>
        <li><strong>Scope Restriction:</strong> Clustering performed exclusively on subreddits in our dataset.</li>
        <li><strong>Recursive Refinement:</strong> Clusters were manually reviewed, and noisy groups were re-processed to achieve finer granularity.</li>
        <li><strong>Representative Sampling:</strong> Only the 250 subreddits closest to each cluster's centroid were retained.</li>
    </ol> 

    <p> <strong>The first improvement</strong> significantly enhanced overall results while reducing 
        the computational load (RAM usage). By filtering out unnecessary data, the number of active 
        subreddits was reduced from ~50,000 to approximately 17,000.
    </p>

    <p> <strong>The second improvement</strong> expanded the diversity of identified groups. 
        By performing a second clustering pass on the previously discarded "noise," we successfully 
        identified 13 additional communities, bringing the total to 36 relevant clusters.
    </p>

    <p> <strong>The third improvement</strong> ensured higher community consistency. Given that 
        the embeddings were dense and spread out, large clusters naturally accumulated noise. 
        To counter this, we focused on the "core" of each community: by retaining only the subreddits 
        closest to the centroids, we preserved the most representative samples. Ultimately, the final 
        dataset consists of 6,562 subreddits distributed across 36 high-cohesion clusters.
    </p>

    <h4>Results (Pass 1)</h4>   
    <div class="ds-grid-2">
        <div>
            <div class="ds-frame" style="height: 300px;">
                <img src="assets/plots/embeddings_and_clustering/1st_clustering_box_no_filter.png" class="ds-img-responsive">
            </div>
            <span class="ds-caption">Distribution before filtering (High Noise)</span>
        </div>
        <div>
            <div class="ds-frame" style="height: 300px;">
                <img src="assets/plots/embeddings_and_clustering/1st_clustering_box_with_filter.png" class="ds-img-responsive">
            </div>
            <span class="ds-caption">Distribution after keeping 250 best subs (Clean)</span>
        </div>
    </div>

    <p> <strong>The initial plot</strong> (left) illustrates the distribution following the first clustering pass. Clusters were ranked by size, 
        ranging from 8,070 subreddits in Cluster 0 to 52 in Cluster 29. A significant volume of data points was identified 
        as outliers due to their high distance from the respective centroids. 
    </p>

    <p> <strong>The second plot</strong> (right) demonstrates a clear improvement following the distance-based pruning. By retaining only 
        the subreddits closest to each centroid, we achieved a substantial noise reduction, with most distances falling 
        below a 0.5 threshold. 
    </p>

    <p> Following this automated refinement, a manual labeling process was conducted. To facilitate this, 
        the ten subreddits closest to each centroid were analyzed for every cluster. This allowed us to validate 
        the thematic consistency of each group and retain only the clusters representing genuine, 
        well-defined communities. 
    </p>

    <!-- Cluster Lists Grid -->
    <div class="ds-grid-2">
        <!-- List of Clusters -->
        <div class="ds-list-box">
            <h4>Relevant clusters and assigned label</h4>
            <div class="ds-list-clean">
                <ul>
                    <li><strong>2:</strong> Gaming / PC</li>
                    <li><strong>4:</strong> Popular / Memes</li>
                    <li><strong>6:</strong> Webmarketing / Dev</li>
                    <li><strong>7 - 21 - 22:</strong> Adult Content</li>
                    <li><strong>9:</strong> Music</li>
                    <li><strong>10:</strong> TV / Movies</li>
                    <li><strong>12:</strong> Feminine Celebrity</li>
                    <li><strong>13 - 14:</strong> Sports (US - Soccer)</li>
                    <li><strong>15:</strong> League of Legends</li>
                    <li><strong>16:</strong> Crypto / Blockchain</li>
                    <li><strong>17:</strong> Fiction / Art</li>
                    <li><strong>18:</strong> My Little Pony</li>
                    <li><strong>19:</strong> YouTube / Creators</li>
                    <li><strong>23:</strong> Japanese Subreddits</li>
                    <li><strong>24:</strong> K-Pop</li>
                    <li><strong>25:</strong> Metafandom</li>
                    <li><strong>26:</strong> Wrestling</li>
                    <li><strong>27:</strong> Retrogaming</li>
                    <li><strong>28:</strong> Vape</li>
                    <li><strong>29:</strong> Educational Video</li>
                </ul>   
            </div>
        </div>
        
        <!-- Examples -->
        <div class="ds-list-box" style="border-color: var(--text-muted);">
            <h4>Examples near centroid</h4>
            <div style="font-size: 0.9em; line-height: 1.6;">
                <p><strong>Cluster 2:</strong> oxygennotincluded, swgemu, speedrunnersgame, pokemmo, hollowknight, necreopolis, darksoulsmods, sky3ds, gits_fa, playdreadnought</p>
                <p><strong>Cluster 9:</strong> soothing, noise, music_share, selfmusic, underground_music, experimental, redditoriginals, glitch, remixes, musicinthemaking</p>
                <p><strong>Cluster 16:</strong> bitcoinuk, counterparty_xcp, trezor, augur, lisk, bitcoin_unlimited, namecoin, primecoin, shadowcash, nubits</p>
            </div>
        </div>  
    </div>

    <p>Then we took all the subreddits from the remaining clusters and ran clustering again leading to these new clusters:</p>

    <!-- 2nd Pass Visualization -->
    <div class="ds-flex-row">
        <div style="flex: 2;">
            <div class="ds-frame" style="height: 400px;">
                <img src="assets/plots/embeddings_and_clustering/2nd_clustering_box_with_filter.png" class="ds-img-responsive">
            </div>
            <span class="ds-caption">Box plots of the clusters generated from noise (2nd Pass)</span>
        </div>

        <div class="ds-list-box" style="flex: 1;">
            <h4>New clusters found</h4>
            <div class="ds-list-clean">
                <ul>
                    <li><strong>100:</strong> R4R / Personals</li>
                    <li><strong>103:</strong> Politics / Academics</li>
                    <li><strong>107:</strong> Adult Content</li>
                    <li><strong>108:</strong> US States and Cities</li>
                    <li><strong>109:</strong> Radical Politics</li>
                    <li><strong>111:</strong> Adult Content</li>
                    <li><strong>112:</strong> India</li>
                    <li><strong>113:</strong> Image Of</li>
                    <li><strong>114:</strong> Germany</li>
                    <li><strong>115:</strong> News Auto</li>
                    <li><strong>116:</strong> Radical Politics</li>
                    <li><strong>117:</strong> Sweden</li>
                    <li><strong>120:</strong> Russia</li>
                </ul>
            </div>
        </div>
    </div>

    <h2>Final community map</h2>
    <div class="ds-frame" style="height: 600px;">
        <iframe src="assets/plots/embeddings_and_clustering/reddit_community_map.html"></iframe>
    </div>
    <span class="ds-caption">Final interactive community map (single or double click on legend to isolate communities)</span>

    <div style="margin-top: 30px;">
        <p><b>Interesting observations:</b></p>
        <p> <strong>First,</strong> most of the clusters are dense and well-separated,
            which successfully achieves our goal of clear topic categorization. 
        </p>

        <p> <strong>Second,</strong> we noticed some interesting outliers within the clusters. When a subreddit 
            seems unrelated but is located at the center of a cluster, it suggests that users from 
            that specific community actively use it, even if the connection isn't obvious at first. 
            However, some points are clearly misplaced: for example, <b>gamingnews</b> was classified 
            in the "My Little Pony" cluster, yet its spatial positioning on the map is between 
            "Retrogaming" and "YouTube/Creators." This visual geography shows where it truly belongs. 
            While a perfect classification is nearly impossible, our objective was to find the best 
            balance between minimizing outliers and retaining as many subreddits as possible. 
        </p>

        <p> <strong>Third,</strong> there is a visible fragmentation within the "Adult Content" category, which is 
            composed of several distinct sub-clusters. This is intentional: to avoid categorizing 
            communities based on specific fetishes, we grouped all pornography-related subreddits 
            under the general "Adult Content" label, even when the algorithm identified distinct 
            sub-communities within that larger group. 
        </p>
    </div>

   <!-- SECTION: LINGUISTIC VARIATION -->
    <div class="ds-section">
        <h2>Exploring Linguistic Qualities by Community</h2>
        <p>
            The data reveals a fascinating spectrum of communication styles. By analyzing the 
            <b>Linguistic Quality Index (LQI)</b> alongside the individual metrics, we can debunk 
            some stereotypes and confirm others.
        </p>
        <p>
            Use the interactive tool below to compare the <b>"Academic"</b> clusters (highest quality) 
            against the <b>"Casual"</b> clusters (lowest quality), or explore the largest communities on Reddit. 
            For better visual clarity, only clusters with more than 500 posts are included in this comparison.
        </p>

        <!-- The Graph Frame (Full Width) -->
        <div class="ds-frame" style="height: 625px;">
            <iframe 
                src="assets/plots/cluster_comparison_normalized2.html" 
                title="Cluster Linguistic Comparison">
            </iframe>
        </div>
        <span class="ds-caption">
            Normalized distribution of linguistic metrics across different community clusters. 
            Y-Axis represents percentage density to allow fair comparison between large and small clusters. (single or double click on legend to isolate clusters)
        </span>

        <!-- START: Side-by-Side Data & Analysis -->
        <div class="ds-grid-2" style="grid-template-columns: auto auto;">
            
            <!-- LEFT: The Data Table -->
            <div class="ds-list-box" style="padding: 0; overflow: hidden; border-left: none; border-top: 4px solid var(--theme-color);">
                <table class="ds-table">
                    <thead>
                        <tr>
                            <th>Cluster Label</th>
                            <th>LQI</th>
                            <th>C.D.</th>
                        </tr>
                    </thead>
                    <tbody>
                        <!-- HIGH LQI SECTION -->
                        <tr><td colspan="3" class="ds-table-header">üèÜ Top 5</td></tr>
                        <tr><td>Hard Videogames</td><td class="val-high">0.654</td><td>0.549</td></tr>
                        <tr><td>Politics/Activism</td><td class="val-high">0.644</td><td>0.600</td></tr>
                        <tr><td>Crypto</td><td class="val-high">0.625</td><td>0.615</td></tr>
                        <tr><td>Tech/Hardware</td><td class="val-high">0.602</td><td>0.527</td></tr>
                        <tr><td>Sports (US)</td><td class="val-high">0.594</td><td>0.469</td></tr>

                        <!-- LOW LQI SECTION -->
                        <tr><td colspan="3" class="ds-table-header" style="background:#ffebee; color:#d32f2f;">üé™ Bottom 5</td></tr>
                        <tr><td>Wrestling</td><td class="val-low">0.473</td><td>0.443</td></tr>
                        <tr><td>German Politics</td><td class="val-low">0.459</td><td>0.520</td></tr>
                        <tr><td>Retrogaming</td><td class="val-low">0.431</td><td>0.477</td></tr>
                        <tr><td>Memes/Oddities</td><td class="val-low">0.393</td><td>0.383</td></tr>
                        <tr><td>Japanese</td><td class="val-low">0.359</td><td>0.370</td></tr>
                    </tbody>
                </table>
            </div>

            <!-- RIGHT: The Narrative Analysis -->
            <div>
                <h4 style="margin-top: 0;">1. The Cr√™me de la cr√™me</h4>
                <p>
                    <b>The Hardcore Gamers:</b> Surprisingly, the #1 spot is <b>Hard Videogames</b>. Unlike casual gaming, these communities probably rely on denser theory-crafting and mechanics analysis.
                </p>
                <p>
                    <b>Crypto Complexities:</b> The <b>Crypto</b> cluster (#3) has the highest <b>Cognitive Depth (0.615)</b> in the entire dataset. This suggests that financial and technical discussions drive complex language even more than <b>Politics</b>, 
                    this might also show that people in such communities have to communicate in a "correct" and "complex" manner in order to belong to their specific social group.".
                </p>

                <h4 style="margin-top: 25px; color: #d32f2f;">2. The lower class</h4>
                <p>
                    <b>Less Formal Communities:</b> <b>Memes/Oddities</b> scores very low (0.393). The data shows low Lexical Richness and Complexity, 
                    indicating usual communications are probably built on simpler and less formally organized language. Even though, 
                    informations density might not be worse than in better scoring communities, the way information is shared is just different.
                </p>
                <p>
                    <b>Artifacts & Other Causes:</b> The <b>Japanese</b> and <b>German Politics</b> clusters score are the lowest probably due to our algorithms struggling with non-English text.
                     Meanwhile, <b>Retrogaming</b> scores surprisingly low compared to modern gaming, likely due to be a more casual community with simpler comments rather than technical analysis.
                </p>
            </div>
        </div>
        <!-- END: Side-by-Side Data & Analysis -->

        <h3>The Linguistic Galaxy</h3>
        <p>
            To visualize the scale of these communities, we mapped the Top 5 and Bottom 5 clusters.
            <br>
            ‚Ä¢ <b>Size (Slice width):</b> Represents the number of posts (Volume).
            <br>
            ‚Ä¢ <b>Color (Red to Blue):</b> Represents the Linguistic Quality (LQI).
        </p>

        <div class="ds-frame" style="max-width: 850px; width: 100%; aspect-ratio: 1 / 1; margin: auto;">
            <iframe 
                src="assets/plots/cluster_sunburst.html" 
                title="Cluster Sunburst Chart"
                style="border: none;">
            </iframe>
        </div>
        
        <span class="ds-caption">
            It's a Sunburst Chart: click on a slice to zoom in. The outer ring show the individual subreddits with at least 15 posts.
        </span>

        <div class="ds-abstract">
            <strong>TLDR;</strong> 
            To explore in a different way the Landscape of Reddit communities, 
            we tried to categorize 476 active subreddits (comunities with at least 200 posts) into a strict hierarchy using only their names inputted into an LLM. 
            We then split every specific topic into above and bellow average named respectively "top" and "bottom" in the graph, this separation based on their LQI scores.
            This allows us to see exactly <em>which</em> subreddits are driving the quality up (or down) within their specific niches.
        </div>

        <p>
            The diagram below is an <strong>Interactive Treemap</strong>. It tries to represent as much of the volume of the dataset 
            but we had to drop a great quantities of small subreddits for clarity and ease of navigation.
            <br><br>
            <strong>How to use it:</strong>
            <br>
            1. <strong>Click on a broad topic</strong> (e.g., "Gaming") to zoom in.
            <br>
            2. <strong>Click on a specific category</strong> (e.g., "Esports & Competitive").
            <br>
            3. You will see the subreddits split into <span style="color: #0d47a1; font-weight:bold;">High Quality (Blue)</span> 
            and <span style="color: #b71c1c; font-weight:bold;">Low Quality (Red)</span> relative to their peers.
        </p>

        <!-- The Graph Frame -->
        <!-- We use a tall height (970px) because the hierarchy is deep -->
        <div class="ds-frame" style="height: 970px;">
            <iframe 
                src="assets/plots/hierarchical_quality_tree.html" 
                title="Hierarchical Quality Tree"
                style="width: 100%; height: 100%; border: none;">
            </iframe>
        </div>
        
        <span class="ds-caption">
            Hierarchical view of Reddit's Linguistic Landscape. Size = Post Volume | Color = LQI Score.
        </span>

        <h3>Key Takeaways from the Data</h3>
        <p style="margin-bottom: 25px;">Click on the cards below to unfold specific anomalies found in the data.</p>

        <div class="insight-stack">

            <!-- CARD 1: TECH -->
            <div class="insight-card">
                <div class="insight-header" onclick="toggleInsight(this)">
                    <h4 class="insight-title">1. The "Code vs. Prose" Paradox</h4>
                    <span class="insight-icon">‚ñº</span>
                </div>
                <div class="insight-body">
                    <div class="insight-content-inner">
                        <p>
                            While <strong>Tech & Science</strong> is the highest scoring category overall (0.621), the internal divide is massive.
                        </p>
                        <ul class="ds-list-clean">
                            <li>
                                <strong>Academic Rigor:</strong> Subreddits like <em>r/badhistory</em> (0.977) and <em>r/askscience</em> (0.915) represent the peak of Reddit's formal linguistic code.
                            </li>
                            <li>
                                <strong>The Utility Drop:</strong> Surprisingly, <em>r/programming</em> (0.396) scores very low. This is most likely a data artifact: our algorithms look for sentence variation and prose. Programming discussions consist of code snippets, error logs, and succinct logic‚Äîformats that lack the specific linguistic structures ("formalities") our metrics are supposed to quantify.
                            </li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- CARD 2: CONSPIRACY -->
            <div class="insight-card">
                <div class="insight-header" onclick="toggleInsight(this)">
                    <h4 class="insight-title">2. The "Pseudo-Intellectual" Trap</h4>
                    <span class="insight-icon">‚ñº</span>
                </div>
                <div class="insight-body">
                    <div class="insight-content-inner">
                        <p>
                            One of the most fascinating findings is the high placement of fringe communities.
                        </p>
                        <p>
                            <em>r/targetedenergyweapons</em> (0.803) scores significantly higher than most mainstream news subreddits. This most likely confirms a linguistic theory: <strong>Conspiracy theorists mimic academic language.</strong> 
                        </p>
                        <p>
                            To sound authoritative, these communities utilize dense vocabulary, complex causality connectors (<em>"consequently," "therefore"</em>), and a detached formal tone. They achieve a high LQI score because the <em>structure</em> of their language is complex, even if the <em>content</em> is factually dubious.
                        </p>
                    </div>
                </div>
            </div>

            <!-- CARD 3: FANS VS ANALYSTS -->
            <div class="insight-card">
                <div class="insight-header" onclick="toggleInsight(this)">
                    <h4 class="insight-title">3. The Tribal Split: Fans vs. Analysts</h4>
                    <span class="insight-icon">‚ñº</span>
                </div>
                <div class="insight-body">
                    <div class="insight-content-inner">
                        <p>
                            Both Gaming and Sports show a clear linguistic divide based on the <em>function</em> of the community:
                        </p>
                        <ul class="ds-list-clean">
                            <li>
                                <strong>The Analysts:</strong> Communities like <em>r/truegaming</em> (0.904) and <em>r/atletico</em> (0.838) are top-tier. They mirror the behavior of the "Hard Videogame" cluster identified in our earlier analysis‚Äîfocusing on theory-crafting and tactics.
                            </li>
                            <li>
                                <strong>The Fans:</strong> Mega-communities like <em>r/pcmasterrace</em> (0.351) and <em>r/lakers</em> (0.423) rank at the bottom. These are "Hype" communities where communication is mostly phatic‚Äîcheering, memes, and short reactions‚Äîprioritizing speed and emotion over structure.
                            </li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- CARD 4: OUTLIER -->
            <div class="insight-card">
                <div class="insight-header" onclick="toggleInsight(this)">
                    <h4 class="insight-title">4. The Surrealist Outlier</h4>
                    <span class="insight-icon">‚ñº</span>
                </div>
                <div class="insight-body">
                    <div class="insight-content-inner">
                        <p>
                            The single highest scoring subreddit in the Entertainment category is... <em>r/wackytictacs</em> (0.954).
                        </p>
                        <p>
                            At first glance, this seems like a bug. It is a meme subreddit. However, it specializes in <strong>"Copypastas"</strong>‚Äîdense blocks of surreal, ironic text. 
                        </p>
                        <div class="ds-todo" style="margin: 10px 0;">
                            To an algorithm, a 500-word block of intense, unique vocabulary looks like "High Quality" writing.
                        </div>
                        <p>
                            This highlights a crucial limitation of NLP: distinguishing between authentic <em>intelligence</em> and algorithmic <em>complexity</em>.
                        </p>
                    </div>
                </div>
            </div>

        </div>

    </div>
</div>

<!-- SECTION: VARIATION ACROSS REDDIT -->
<div class="ds-section">
    <h1>5. Linguistic variation across Reddit</h1>
    
</div>

<!-- SECTION: NEGATIVITY -->
<div class="ds-section">
    <h1>6. Linguistic quality and negativity</h1>
    
    <p>
        A common assumption is that negative content is "lower quality"‚Äîthink of flame wars, trolls, and insults. 
        However, our data tells a more complex story.
    </p>

    <div class="ds-grid-2">
        <div class="ds-blockquote">
            "Anger does not equal stupidity. In fact, some of the most linguistically complex posts in our dataset were highly negative rants."
        </div>
        <div>
            <p>
                When we plotted LQI against sentiment, we found that extremely negative posts often had <strong>higher</strong> 
                structural complexity than neutral posts. Anger on Reddit often manifests as detailed, itemized argumentation.
                The "dumbest" content (lowest LQI) was actually found in the "casual positive" quadrant‚Äîshort, low-effort agreement comments like "This is awesome!" or "lol".
            </p>
        </div>
    </div>
</div>

<!-- SECTION: CONCLUSION -->
<div class="ds-section">
    <h1>7. Conclusion</h1>
    <p>
        So, who wins the "War of the Learned"? Our analysis shows that Reddit is not a monolith of poor grammar, 
        nor is it a citadel of high intellect. It is a federation of digital city-states, each with its own dialect.
    </p>
    <p>
        While academic and political communities maintain rigorous standards of Formal Code, other communities have optimized 
        their language for speed and connection (Contextual Code). Defining one as "better" ignores the function of language. 
        However, if you are looking for complex sentence structures and varied vocabulary, you are more likely to find it in a 
        heated political debate than in a friendly fan appreciation thread.
    </p>
</div>

<!-- ======================================================= -->
<!-- SCRIPTS                                                 -->
<!-- ======================================================= -->
<script>
    function toggleMetric(element) {
        // Toggle the active class on the parent item
        const item = element.parentElement;
        item.classList.toggle('active');

        // Handle the max-height for the slide effect
        const content = item.querySelector('.metric-content');
        if (content.style.maxHeight) {
            content.style.maxHeight = null;
        } else {
            content.style.maxHeight = content.scrollHeight + "px";
        }
    }

    function toggleInsight(headerElement) {
        // Identify the parent card
        const card = headerElement.parentElement;
        
        // Check if it is currently active
        const isActive = card.classList.contains('active');

        // Optional: Close all other cards first (Accordion behavior)
        // If you want multiple to stay open, delete this block
        const allCards = document.querySelectorAll('.insight-card');
        allCards.forEach(c => {
            c.classList.remove('active');
            c.querySelector('.insight-body').style.maxHeight = null;
        });

        // Toggle the clicked card
        if (!isActive) {
            card.classList.add('active');
            const body = card.querySelector('.insight-body');
            body.style.maxHeight = body.scrollHeight + "px";
        }
    }
</script>