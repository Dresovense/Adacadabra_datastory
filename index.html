---
layout: page
title: The War of The Learned
subtitle: Data-driven Linguistic Quality Analysis based on Reddit data
---

<!-- ======================================================= -->
<!-- GLOBAL STYLING SYSTEM                                   -->
<!-- Change --theme-color here to update the whole page      -->
<!-- ======================================================= -->
<style>
    :root {
        /* CORE THEME COLORS */
        --theme-color: #11224d;
        /* The main Deep Blue */
        --theme-light: #e6edff;
        /* Very light blue for backgrounds */
        --theme-accent: #f57f17;
        /* Orange for alerts/todos */

        /* TEXT & LAYOUT */
        --text-main: #333333;
        --text-muted: #555555;
        --border-radius: 8px;
        --spacing-section: 80px;
        --spacing-item: 30px;
    }

    /* --- LAYOUT UTILITIES --- */
    .ds-section {
        margin-bottom: var(--spacing-section);
        padding-top: 30px;
        border-top: 1px solid #eee;
        color: var(--text-main);
        line-height: 1.6;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
        text-align: justify;
    }

    .ds-section:first-of-type {
        border-top: none;
        padding-top: 0;
    }

    .ds-grid-2 {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: var(--spacing-item);
        margin: var(--spacing-item) 0;
    }

    .ds-flex-row {
        display: flex;
        gap: var(--spacing-item);
        align-items: flex-start;
        margin: var(--spacing-item) 0;
    }

    /* --- VISUAL COMPONENTS --- */

    /* 1. Plot Frames */
    .ds-frame {
        width: 100%;
        /* border: 2px solid var(--theme-color);  */
        border-radius: var(--border-radius);
        background: white;
        overflow: hidden;
        margin-bottom: 10px;
        box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        display: flex;
        align-items: center;
        justify-content: center;
        box-sizing: border-box;
        position: relative;
    }

    .ds-frame iframe {
        width: 100%;
        height: 100%;
        border: none;
        display: block;
    }

    .ds-caption {
        font-size: 0.9em;
        font-style: italic;
        color: var(--text-muted);
        text-align: center;
        margin-top: 8px;
        display: block;
    }

    /* 2. Images & Reddit Posts */
    .ds-img-responsive {
        max-width: 100%;
        height: auto;
        object-fit: contain;
    }

    .ds-reddit-post {
        display: block;
        margin: var(--spacing-item) auto var(--spacing-item) auto;
        max-width: 60%;
        border-radius: var(--border-radius);
        box-shadow: 0 10px 30px rgba(0, 0, 0, 0.15);
        border: 1px solid rgba(0, 0, 0, 0.05);
    }

    /* 3. Quotes */
    .ds-blockquote {
        border-left: 6px solid var(--theme-color);
        background-color: #f8f9fa;
        padding: 20px 25px;
        margin: 30px 0;
        font-style: italic;
        color: #444;
        border-radius: var(--border-radius);
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
        font-size: 1.05em;
        line-height: 1.7;
    }

    /* 4. Lists, Clusters & Tables */
    .ds-list-box {
        background: #f8f9fa;
        padding: 20px;
        border-radius: var(--border-radius);
        border-left: 5px solid var(--theme-color);
        height: fit-content;
        box-sizing: border-box;
    }

    .ds-list-clean ul {
        list-style: none;
        padding: 0;
        margin: 0;
    }

    .ds-list-clean li {
        padding: 6px 0;
        border-bottom: 1px solid #e0e0e0;
        font-size: 0.95em;
    }

    .ds-list-clean li:last-child {
        border-bottom: none;
    }

    .ds-list-clean strong {
        color: var(--theme-color);
        display: inline-block;
        min-width: 30px;
    }

    /* Data Tables inside list boxes */
    .ds-table {
        width: 100%;
        border-collapse: collapse;
        font-size: 0.85em;
    }

    .ds-table th {
        color: #333;
        padding: 8px;
        text-align: left;
        font-weight: bold;
        border: none;
    }

    .ds-table td {
        padding: 8px;
        border: none;
    }

    .ds-table tr:last-child td {
        border: none;
    }

    .ds-table-header {
        border: none;
        background: #e9ecef;
        color: #555;
        font-weight: bold;
        text-align: center;
        padding: 6px;
        font-size: 0.85em;
        letter-spacing: 1px;
        text-transform: uppercase;
    }

    .val-high {
        color: var(--theme-color);
        font-weight: 700;
        border: 0 1px 0 1px;
    }

    .val-low {
        color: #d32f2f;
        font-weight: 700;
        border: none;
    }

    /* 5. Abstracts & Todos */
    .ds-abstract {
        background-color: var(--theme-light);
        border-left: 6px solid var(--theme-color);
        padding: 20px;
        border-radius: var(--border-radius);
        margin: var(--spacing-item) 0 var(--spacing-item) 0;
        color: var(--text-main);
    }

    .ds-todo {
        background-color: #fff9c4;
        border: 1px dashed var(--theme-accent);
        color: #444;
        padding: 15px;
        margin: 20px 0;
        font-family: monospace;
        font-size: 0.9em;
    }

    /* 6. Metric Accordion Styling */
    .accordion-wrapper {
        border-left: 6px solid #0b1e47;
        background-color: #f8f9fa;
        padding: 20px;
        margin: 25px 0;
        border-radius: var(--border-radius);
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
    }

    .metric-item {
        background-color: white;
        border: 1px solid #e0e0e0;
        border-radius: var(--border-radius);
        margin-bottom: 10px;
        overflow: hidden;
        transition: box-shadow 0.2s;
    }

    .metric-item:hover {
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
    }

    .metric-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 15px;
        cursor: pointer;
        background-color: #fff;
    }

    .metric-header:hover {
        background-color: #f9f9f9;
    }

    .metric-name {
        font-weight: bold;
        font-size: 1.1em;
        color: var(--theme-color);
    }

    .metric-arrow {
        color: var(--theme-color);
        transition: transform 0.3s;
    }

    .metric-item.active .metric-arrow {
        transform: rotate(180deg);
    }

    .metric-content {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.3s ease-out;
        padding: 0 15px;
    }

    .metric-inner {
        padding: 15px 0;
        font-size: 0.95em;
        color: #333;
        border-top: 1px solid #eee;
    }

    .metric-formula {
        background-color: #f1f3f5;
        padding: 4px 8px;
        border-radius: 4px;
        font-family: monospace;
        font-size: 0.9em;
        color: #d63384;
        display: inline-block;
        margin-bottom: 8px;
    }

    /* Container for the stack */
    .insight-stack {
        display: flex;
        flex-direction: column;
        gap: 15px;
        /* Space between boxes */
        margin: 40px 0;
    }

    /* Individual Card Style */
    .insight-card {
        background: white;
        border-radius: var(--border-radius);
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
        /* border: 1px solid #e0e0e0; */
        border: none;
        overflow: hidden;
        /* Hides content when closed */
        transition: all 0.3s ease;
    }

    /* The Clickable Header */
    .insight-header {
        padding: 20px 25px;
        background: white;
        cursor: pointer;
        display: flex;
        justify-content: space-between;
        align-items: center;
        border-left: 6px solid var(--theme-color);
        /* The colored strip */
        transition: background-color 0.2s;
    }

    .insight-header:hover {
        background-color: #f8f9fa;
    }

    .insight-title {
        font-weight: 700;
        font-size: 1.1em;
        color: var(--text-main);
        margin: 0;
    }

    .insight-icon {
        font-size: 1.2em;
        color: var(--theme-color);
        transition: transform 0.3s ease;
    }

    /* The Unfolding Content */
    .insight-body {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.4s ease-out, opacity 0.3s ease;
        opacity: 0;
        background-color: #fafbfc;
        border-top: 1px solid transparent;
    }

    .insight-content-inner {
        padding: 25px;
        color: var(--text-muted);
        font-size: 0.95em;
    }

    /* ACTIVE STATE (When open) */
    .insight-card.active {
        box-shadow: 0 8px 20px rgba(0, 0, 0, 0.1);
        border-color: var(--theme-color);
    }

    .insight-card.active .insight-header {
        background-color: var(--theme-light);
    }

    .insight-card.active .insight-icon {
        transform: rotate(180deg);
        /* Flip arrow */
    }

    .insight-card.active .insight-body {
        opacity: 1;
        border-top-color: #eee;
    }

    /* Custom borders for different topics to add variety */
    .insight-card:nth-child(1) .insight-header {
        border-left-color: #2196F3;
    }

    /* Blue for Tech */
    .insight-card:nth-child(2) .insight-header {
        border-left-color: #673AB7;
    }

    /* Purple for Conspiracy */
    .insight-card:nth-child(3) .insight-header {
        border-left-color: #4CAF50;
    }

    /* Green for Sports */
    .insight-card:nth-child(4) .insight-header {
        border-left-color: #F44336;
    }

    /* Red for Outliers */

    /* RESPONSIVE */
    @media (max-width: 768px) {

        .ds-grid-2,
        .ds-flex-row {
            grid-template-columns: 1fr;
            flex-direction: column;
        }

        .ds-reddit-post,
        .ds-img-main {
            max-width: 100%;
        }
    }
</style>

<!-- ======================================================= -->
<!-- CONTENT START                                           -->
<!-- ======================================================= -->

<a href="https://www.reddit.com/r/TheoryOfReddit/comments/1pcic9v/is_it_just_me_or_has_the_baseline_quality_of/"
    target="_blank">
    <img src="assets/reddit_posts/real_post_1.png" class="ds-reddit-post">
</a>

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- SECTION: INTRO -->
<div class="ds-section">
    <!-- Abstract Box -->
    <div class="ds-abstract">
        <strong>TLDR;</strong> Is the internet really getting "dumber", or is it just getting more diverse? We analyzed
        millions of Reddit posts using 300-dimensional embeddings and linguistic algorithms to map the "Digital
        Dialects" of the internet. We found that while some communities thrive on simplicity, high-complexity language
        is alive and well‚Äîoften in the most unexpected places.
    </div>

    <!--     <p>
        It's not the first time I've seen such sentiments around here. Actually,
        I find it super interesting, as such discussions about "linguistic quality" are not recent.
        In fact you can find them pretty much at any point since we've begun to standardise our languages. 
        Still, with the rise of social media and instant messaging, such perceptions have been on the rise
        and it could be interesting to know if they are based on actual data. And
        if so, what motivates the difference of language quality between users.
    </p> -->

    <p>
        The discussion about language quality and its downfall is not recent and has been ever present since
        humans first started to talk. Just like Moli√®re's writing was not considered, at his time, to be of the same
        quality as his peers,
        he is now considered the standard of the french language, showing how standards evolve over time. However, 
        it is interesting to note that while social media platforms
        have always been the target of mockery when talking about language quality, we do see sentiments
        of users criticising its downfall within the platform itself. Are these concerns verifiable, or can
        high-quality language still be found within Reddit?
    </p>

    <div class="ds-blockquote">
        <strong>Research Questions:</strong>
        <ol>
            <li>How to quantify linguistic quality? Which metrics can reliably be used?</li>
            <li>To what extent linguistic quality varies between Reddit users?</li>
            <li>How does linguistic quality vary across broad subreddit communities? Which stand out
                in terms of higher or lower quality and why?
            </li>
            <li>How does linguistic quality relate to sentiment and tone across Reddit? In particular,
                is there a link between negative sentiment, insults and hostility and language quality?
            </li>
        </ol>
    </div>

    <!-- <div class="ds-todo">
        [Optional: We can formalize Research Questions (RQ1, RQ2) here if we want a more academic tone, or leave it narrative.]
    </div> -->
</div>

<!-- SECTION: DATASET -->
<div class="ds-section">
    <h1>1. Dataset</h1>

    <div class="ds-abstract">
        <strong>TLDR;</strong>  
        Before presenting the metrics that can be used, we first need to
        understand what kind of data we are dealing with. This linguistic
        analysis is based on two reddit datasets. The first deals with
        hyperlinks between subreddits, while the second is a vectorial embeddings
        of users and subreddits. Both datasets can be found <a href="./datasets">here</a>.
    </div>
    
    <h2>Hyperlinks</h2>
    <p>
        Our primary dataset consists of the "Subreddit Hyperlinks Network." This is a massive collection 
        of posts where one subreddit links to another. While it does not provides us with the raw text body of the
        posts, it does
        provide timestamps, sentiment labels and 86 different linguistic metrics (from low-level
        metrics such as the number of characters in the post, to high-end metrics derived from <a
            href="https://www.liwc.app/">LIWC</a>, a state-of-the-art linguistics model),
         allowing us to analyze in-depth the way users express themselves.
    </p>

    <h2>Embeddings</h2>
    
    <!-- <h3>Introduction</h3> -->
    <p>
        Along with the initial subreddit hyperlinks dataset, we used another dataset: 
        the embedding vectors of subreddits.
    </p>
    <p>
        These embeddings are high-dimensional vectors (in our case, 300 dimensions).
        They are designed to represent similarities between data points in a complex space.
        Specifically, each of the ~50,000 subreddits is assigned a 300-dimensional vector.
        These vectors indicate similarity based on user behavior: if many users post in
        the same group of subreddits, those subreddits will be closer to each other in this
        300-dimensional space.
    </p>
    <p>
        These embeddings are very useful for our research on linguistic quality and our goal to determine 
        "who speaks the best". They allow us to identify clusters of subreddits where users share similar 
        interests, effectively grouping subreddits into distinct communities.
    </p>

    <h3>Weakness</h3>
    <p>
        While these embeddings are powerful and perform well, the dataset faces challenges due to the
        nature of Reddit communities and the high number of data.
    </p>

    <div class="ds-blockquote">
        "One might assume that a dataset of 50,000 subreddits is relatively small. However, each
        entry is a 300-dimensional vector... Calculating the similarity between every possible pair
        of subreddits is a computationally intensive task...
        To overcome this, we implemented a batch processing approach."
    </div>

    <p>
        Indeed communities are not perfectly separated into 'clean' clusters where we can easily say:
        'These users are only interested in politics.' Most users have diverse interests and post in a wide
        variety of subreddits. This creates a significant amount of overlap between vectors. Consequently,
        some subreddits appear very close to many others in the 300-dimensional space, which can create 'noisy'
        links and blur the boundaries between different communities.
    </p>

    <h3>Visualizing the Embedding Weaknesses</h3>
    <p>
        The following graphs illustrate the "fuzzy" nature of these communities.
        On <b>Figure 1</b>, a 2D projection (using dimensionality reduction) of the 50,000 subreddits shows the 300-dimensional vectors
        in a 2D-space. Each point represents a subreddit (put your mouse on a point to see which one). We see that the 
        subreddits are not clustered into distinct groups, but rather
        forming a large, dense circle with significant overlap.  We can still identify isolated clusters like on (5, -5) which is quite defined.
        By zooming and looking at subreddits names, we can see that it should be related to pop culture or video-games with subreddits
        like <b>capecodegaming</b>, <b>interactivecinema</b> or <b>virtualrealitygaming</b>. Nevertheless it is hard to confirm this since 
        there is a lot of noise. There is also subreddits as <b>snowskating</b> or <b>abstraction</b>. This visualization confirms that many communities
        are not clearly separated.
        
    </p>
    <p>
        On <b>Figure 2</b>, the graph displays the distribution of close neighbors. The x-axis represents the number of 
        neighbors as the y-axis shows the number of subreddits.  
        A close neighbor is defined using cosine similarity. Cosine similarity measures the orientation of two vectors
         in the 300-dimensional space, ranging from -1 (opposite)
        to 1 (identical). In order to be closed, a neighbor needs 80% similarity. 
         The distribution reveals a non-negligible number of subreddits that have more than 10,000
        neighbors
        with over 80% similarity.
    </p>

    <!-- PLOT 1 -->
    <div style="margin-bottom: 40px;">
        <div class="ds-frame" style="height: 600px;">
            <iframe src="assets/plots/embeddings_and_clustering/reddit_map.html"></iframe>
        </div>
        <span class="ds-caption">Figure 1: 2D Projection of Subreddits embeddings.  (The "Blob" of Reddit)</span>
    </div>

    <!-- PLOT 2 -->
    <div style="margin-bottom: 40px;">
        <div class="ds-frame" style="height: 500px;">
            <img src="assets/plots/embeddings_and_clustering/distribution_of_closed_neigbors_embeddings.png"
                class="ds-img-responsive">
        </div>
        <span class="ds-caption">Figure 2: Distribution of Close Neighbors</span>
    </div>
</div>

<!-- SECTION: DEFINING LANGUAGE -->
<div class="ds-section">
    <h1>2. What Is Language Quality?</h1>
    <div class="ds-abstract">
        <strong>TLDR;</strong>
        The first thing we need to know when trying to understand language quality
        on reddit, is to actually define what language quality <em>is</em>. We can
        define language quality of a text as "<b>the similarity between a text and
            a shared construction of the "perfect" language</b>".
        However, we cannot apply directly such a definition to our data. Instead we need to
        find a way to operationalise such a concept. Thus, we decided to take an
        approach that combined both what our data provided and what different measures
        suggested that this "shared construction" is, arriving at our current metrics.
    </div>

    <!-- START: Metric Accordion -->
    <div class="accordion-wrapper">
        <p style="margin-top:0;">To calculate the <b>Linguistic Quality Index (LQI)</b>, we aggregated four robust
            dimensions. Click below to see the exact formulas derived from our data processing.</p>

        <div class="metric-list">

            <!-- Metric 1 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Lexical Richness (Herdan's C)</span><span class="metric-arrow">‚ñº</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">log(Unique Words) / log(Total Words)</div>
                        <p>Standard Type-Token Ratio is biased against long texts (the longer you write, the more you
                            repeat common words). We used Herdan's C (Log-TTR) to ensure fair comparison between short
                            comments and long rants.</p>
                    </div>
                </div>
            </div>

            <!-- Metric 2 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Structural Complexity</span><span class="metric-arrow">‚ñº</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">(0.5 * Avg Word Len) + log(Avg Sentence Len)</div>
                        <p>Simple sentence length is noisy (a 50-word list of groceries is not "complex"). We created a
                            composite score that rewards using longer, more complex words <i>within</i> structurally
                            longer sentences.</p>
                    </div>
                </div>
            </div>

            <!-- Metric 3 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Formality Index</span><span class="metric-arrow">‚ñº</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">Articles - Pronouns - (2 * Swearing) - Uppercase</div>
                        <p>Based on Heylighen & Dewaele (2002). This distinguishes "Contextual" language (casual,
                            chatty, subjective: "I think...") from "Formal" language (objective, noun-heavy: "The data
                            suggests...").</p>
                    </div>
                </div>
            </div>

            <!-- Metric 4 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Cognitive Depth</span><span class="metric-arrow">‚ñº</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">Insight + 2 * Exclusivity + 5 * Tentativeness + Integration -
                            Certainty + 10 * log(Long Words Frequency) + 5 * Numerical Concepts</div>
                        <p>Quantifies intellectual nuance. It rewards "Distinction Markers" (e.g., <i>but</i>,
                            <i>however</i>) and "Tentative Language" (e.g., <i>perhaps</i>, <i>likely</i>) while
                            penalizing "Certainty
                            Markers" (e.g., <i>always</i>, <i>never</i>).
                        </p>
                    </div>
                </div>
            </div>

        </div>

        The composite LQI is calculated as a weighted sum, prioritizing Cognitive Depth and Formality as the primary
        discriminators of "learned" discourse:

        <p>
            $$LQI_{raw} = 4 \cdot C_D + 3 \cdot F_I + 2 \cdot S_C + 1 \cdot L_R$$
        </p>

        <p>
            <b>Why this weighting?</b> By up-weighting <b>Cognitive Depth (4)</b> and <b>Formality (3)</b>, we define
            quality as <i>dispassionate analysis</i>. This prevents emotional "rants"‚Äîwhich may be structurally complex
            but highly subjective‚Äîfrom scoring highly, instead rewarding content that is nuanced, objective, and
            precise.
        </p>

        The final score is quantile-clipped (5th-95th percentile) and MinMax scaled to \([0, 1]\).
    </div>
    <!-- END: Metric Accordion -->

    <h3>Feature Analysis</h3>
    <div class="ds-frame" style="height: 530px;">
        <iframe src="assets/plots/linguistic_feature_histogram.html"></iframe>
        <span class="ds-caption">Figure 3: Feature Density</span>
    </div>

    <p>
        The histograms above show distinct characteristics. For example, <b>lexical_richness</b> shows a sharp spike at
        1.0. This artifact represents very short posts (e.g., "Yes", "lol") where every word is unique. This confirms
        why simple ratios fail and why we need robust metrics.
        Meanwhile, <b>cognitive_depth</b> often spikes at zero, indicating that a significant portion of Reddit
        communication is purely phatic or descriptive, lacking explicit reasoning words.
    </p>

    <div class="ds-grid-2">
        <div>
            <img src="assets/plots/feature_correlation_matrix.png" class="ds-img-responsive" style="border-radius:8px;">
            <span class="ds-caption">Figure 4: Correlation Matrix of Linguistic Features</span>
        </div>
        <div>
            <h4>Orthogonality of Features</h4>
            <p>
                An essential check was to ensure our metrics weren't just measuring the same thing four times.
                As the correlation matrix shows, our chosen features have low overlap.
            </p>
            <!-- <p>
                Interestingly, <b>Lexical Richness</b> is negatively correlated with <b>Syntactic Complexity</b>. This makes sense: 
                complex academic texts often reuse specific terminology (low richness) within very long, complex sentences. 
                This confirms that each feature captures a unique dimension of the "Linguistic Profile."
            </p> -->
            <p>
                Interestingly, <b>Structural Complexity</b> and <b>Cognitive Depth</b> have
                the highest amount of correlation between our four metrics. While it is low enough
                to understand that they measure different dimensions of the "linguisic profile", it can
                be explained from the fact that posts with a complex argument will tend to
                make use of more complex words and sentence structures.
            </p>
            <p>
                Moreover, the correlation between <b>LQI</b> and
                different sentiment metrics (such as Link_Sentiment, LIWC_Swear and vader_compound) is quite low.
                This will be subject to an in-depth analysis in section 6, but
                it already hints to a low influence of sentimentality on linguistic quality.
            </p>
        </div>
    </div>
</div>

<!-- SECTION: SUBREDDIT QUALITY -->
<div class="ds-section">
    <h1>3. Linguistic Quality Within Specific Subreddits</h1>

    <!-- <div class="ds-abstract"> -->
        <!-- <strong>TLDR;</strong> -->
        <p>Let's try to validate our instincts by checking up on the "best" and "worst" performing subreddits.</p>
    <!-- </div> -->

    <!-- Interactive Plot -->
    <div class="ds-frame" style="height: 630px;">
        <iframe src="assets/plots/metric_distributions.html"></iframe>
    </div>
    <span class="ds-caption">
            Figure 5: Normalized distribution of linguistic metrics from individual subreddits. 
        Y-Axis represents percentage density to allow fair comparison between with differently sized subs with at least
        500 posts. (single or double click on legend to isolate subreddits)
    </span>

            <!-- START: Side-by-Side Data & Analysis -->
            <div class="ds-grid-2" style="grid-template-columns: auto auto auto;">
            
                <!-- LEFT: The Data Table (HIGH LQI) -->
        <div class="ds-list-box"
            style="padding: 0; overflow: hidden; border-left: none; border-top: 4px solid var(--theme-color);">
                    <table class="ds-table">
                        <thead>
                            <tr>
                                <th>Subreddit</th>
                                <th>LQI</th>
                            </tr>
                        </thead>
                        <tbody>
                            <!-- HIGH LQI SECTION -->
                    <tr>
                        <td colspan="2" class="ds-table-header">üèÜ Top 10</td>
                    </tr>
                    <tr>
                        <td>r/badhistory</td>
                        <td class="val-high">0.977</td>
                    </tr>
                    <tr>
                        <td>r/trendingsubreddits</td>
                        <td class="val-high">0.939</td>
                    </tr>
                    <tr>
                        <td>r/askhistorians</td>
                        <td class="val-high">0.914</td>
                    </tr>
                    <tr>
                        <td>r/dailydot</td>
                        <td class="val-high">0.870</td>
                    </tr>
                    <tr>
                        <td>r/undelete</td>
                        <td class="val-high">0.829</td>
                    </tr>
                    <tr>
                        <td>r/openbroke</td>
                        <td class="val-high">0.824</td>
                    </tr>
                    <tr>
                        <td>r/subredditoftheday</td>
                        <td class="val-high">0.809</td>
                    </tr>
                    <tr>
                        <td>r/explainlikeimfive</td>
                        <td class="val-high">0.806</td>
                    </tr>
                    <tr>
                        <td>r/karmacourt</td>
                        <td class="val-high">0.792</td>
                    </tr>
                    <tr>
                        <td>r/nostupidquestions</td>
                        <td class="val-high">0.785</td>
                    </tr>
                        </tbody>
                    </table>
                </div>

                <!-- Middle: The Data Table (LOW LQI) -->
        <div class="ds-list-box"
            style="padding: 0; overflow: hidden; border-left: none; border-top: 4px solid var(--theme-color);">
                    <table class="ds-table">
                        <thead>
                            <tr>
                                <th>Subreddit</th>
                                <th>LQI</th>
                            </tr>
                        </thead>
                        <tbody>    
                            <!-- LOW LQI SECTION -->
                    <tr>
                        <td colspan="2" class="ds-table-header" style="background:#ffebee; color:#d32f2f;">üé™ Bottom 5
                        </td>
                    </tr>
                    <tr>
                        <td>r/bestoftldr</td>
                        <td class="val-low">0.188</td>
                    </tr>
                    <tr>
                        <td>r/peoplewhosayheck</td>
                        <td class="val-low">0.223</td>
                    </tr>
                    <tr>
                        <td>r/soposts</td>
                        <td class="val-low">0.230</td>
                    </tr>
                    <tr>
                        <td>r/evenwithcontext</td>
                        <td class="val-low">0.247</td>
                    </tr>
                    <tr>
                        <td>r/shitpost</td>
                        <td class="val-low">0.252</td>
                    </tr>
                    <tr>
                        <td>r/gamingcirclejerk</td>
                        <td class="val-low">0.273</td>
                    </tr>
                    <tr>
                        <td>r/fitnesscirclejerk</td>
                        <td class="val-low">0.297</td>
                    </tr>
                    <tr>
                        <td>r/negativewithgold</td>
                        <td class="val-low">0.300</td>
                    </tr>
                    <tr>
                        <td>r/againskarmawhores</td>
                        <td class="val-low">0.330</td>
                    </tr>
                    <tr>
                        <td>r/japancirclejerk</td>
                        <td class="val-low">0.331</td>
                    </tr>
                        </tbody>
                    </table>
                </div>
    
                <!-- RIGHT: The Narrative Analysis -->
                <div>
                    <p><b>Analysis of the Leaderboard:</b></p>
                    <p>
                        The distinction is stark. The top-performing subreddits are dominated by specific, 
                knowledge-heavy communities like <i>r/AskHistorians</i> or <i>r/explainlikeimfive</i>. These communities
                enforce strict
                moderation policies that require detailed, cited responses, naturally inflating their Structural
                Complexity and Cognitive Depth scores.
                    </p>
                    <p>
                Conversely, the bottom-performing subreddits are populated by meme subreddits and rapid-fire gaming
                communities.
                However, "low quality" here doesn't necessarily mean "stupid." It often reflects a different
                <i>function</i>
                of language: efficient, high-context communication (memes, slang) that prioritizes speed and communities
                dependent text structures over formalities.
                    </p>
                </div>
            </div>
            <!-- END: Side-by-Side Data & Analysis -->


</div>

<!-- SECTION: CLUSTERING / COMMUNITY -->
<div class="ds-section">
    <h1>4. Linguistic variation within a topic/community</h1>
    <!-- <img src="assets/reddit_posts/comment_1.png" class="ds-reddit-post"> -->

    <!-- <h2>Introduction</h2> -->

    <div class="ds-abstract">
        <strong>TLDR;</strong>
        What is a community? Oxford Languages defines it as: 'a group of people
        living in the same place or having a particular characteristic in common.'
        Applying this to Reddit, we can define a community as a group of users sharing a
        common interest. In this section, our objective is to apply clustering algorithms
        to the subreddit embeddings. By doing so, we aim to group subreddits with similar
        vector representations into distinct clusters, effectively mapping out digital communities
        based on shared user interests.
    </div>

    <h2>Methodology</h2>
    <h3>Clustering Strategy and Challenges</h3>

    <p>
        The primary challenge in this analysis stems from "<strong>bridges</strong>": 
        subreddits frequented by users from vastly different backgrounds. These bridges 
        exhibit a high number of neighbors, making them notoriously difficult to classify. 
        In standard clustering, these points either force the creation of massive, noisy clusters 
        (K-means) or are discarded entirely as outliers (HDBSCAN).
    </p>

    <div class="ds-list-box" style="margin-bottom: 20px;">
        <div class="ds-list-clean">
            <ul>
                <li>
                    <strong>Why K-means Failed:</strong> Our initial attempt using <strong>K-means</strong> proved
                    inadequate.
                    K-means inherently assumes spherical (circular) cluster shapes and requires
                    <i>a-priori</i> knowledge of the exact number of clusters. Given the nature of
                    Reddit, communities are not perfectly circular; they overlap extensively due to the diverse
                    interests of users. Forcing these high-dimensional "clouds" into rigid spheres resulted
                    in a poor representation of the social reality.
                </li>
                <li>
                    <strong>The Density Limitation of HDBSCAN:</strong> was a logical next step due to its ability to
                    handle
                    non-spherical shapes. However, it proved too selective for our dataset.
                    While it identified high-density cores, the resulting clusters were too
                    small, and a significant portion of the subreddits were labeled as outliers.
                </li>
            </ul>
        </div>
    </div>

    <h3>Refined Clustering Approach</h3>
    <p>
        At this stage, we realized our fundamental assumption was flawed: we were
        looking for <strong>predefined topics</strong> (e.g., "politics", "video games").
        In reality, embeddings capture <strong>community behaviors</strong>. A cluster
        might not represent a single subject, but rather a specific demographic of
        users who share multiple interests.
    </p>

    <p>To refine the methodology, three key improvements were implemented:</p>
    <ol>
        <li><strong>Scope Restriction:</strong> Clustering performed exclusively on subreddits in our dataset.</li>
        <li><strong>Recursive Refinement:</strong> Clusters were manually reviewed, and noisy groups were re-processed
            to achieve finer granularity.</li>
        <li><strong>Representative Sampling:</strong> Only the 250 subreddits closest to each cluster's centroid were
            retained.</li>
    </ol>

    <p> <strong>The first improvement</strong> significantly enhanced overall results while reducing
        the computational load (RAM usage). By filtering out unnecessary data, the number of active
        subreddits was reduced from ~50,000 to approximately 17,000.
    </p>

    <p> <strong>The second improvement</strong> expanded the diversity of identified groups.
        By performing a second clustering pass on the previously discarded "noise," we successfully
        identified 13 additional communities, bringing the total to 36 relevant clusters.
    </p>

    <p> <strong>The third improvement</strong> ensured higher community consistency. Given that
        the embeddings were dense and spread out, large clusters naturally accumulated noise.
        To counter this, we focused on the "core" of each community: by retaining only the subreddits
        closest to the centroids, we preserved the most representative samples. Ultimately, the final
        dataset consists of 6,562 subreddits distributed across 36 high-cohesion clusters.
    </p>

    <h4>Results (Pass 1)</h4>
    <div class="ds-grid-2">
        <div>
            <div class="ds-frame" style="height: 300px;">
                <img src="assets/plots/embeddings_and_clustering/1st_clustering_box_no_filter.png"
                    class="ds-img-responsive">
            </div>
            <span class="ds-caption">Figure 6: Distribution before filtering (High Noise)</span>
        </div>
        <div>
            <div class="ds-frame" style="height: 300px;">
                <img src="assets/plots/embeddings_and_clustering/1st_clustering_box_with_filter.png"
                    class="ds-img-responsive">
            </div>
            <span class="ds-caption">Figure 7: Distribution after keeping 250 best subs (Clean)</span>
        </div>
    </div>

    <p> <strong>Figure 6</strong> illustrates the distribution following the first clustering pass. Clusters were ranked by size, 
        ranging from 8,070 subreddits in Cluster 0 to 52 in Cluster 29. A significant volume of data points was identified 
        as outliers due to their high distance from the respective centroids. 
    </p>

    <p> <strong>Figure 7</strong> demonstrates a clear improvement following the distance-based pruning. By retaining only 
        the subreddits closest to each centroid, we achieved a substantial noise reduction, with most distances falling 
        below a 0.5 threshold. 
    </p>

    <p> Following this automated refinement, a manual labeling process was conducted. To facilitate this,
        the ten subreddits closest to each centroid were analyzed for every cluster. This allowed us to validate
        the thematic consistency of each group and retain only the clusters representing genuine,
        well-defined communities.
    </p>

    <!-- Cluster Lists Grid -->
    <div class="ds-grid-2">
        <!-- List of Clusters -->
        <div class="ds-list-box">
            <h4>Relevant clusters and assigned label</h4>
            <div class="ds-list-clean">
                <ul>
                    <li><strong>2:</strong> Gaming / PC</li>
                    <li><strong>4:</strong> Popular / Memes</li>
                    <li><strong>6:</strong> Webmarketing / Dev</li>
                    <li><strong>7 - 21 - 22:</strong> Adult Content</li>
                    <li><strong>9:</strong> Music</li>
                    <li><strong>10:</strong> TV / Movies</li>
                    <li><strong>12:</strong> Feminine Celebrity</li>
                    <li><strong>13 - 14:</strong> Sports (US - Soccer)</li>
                    <li><strong>15:</strong> League of Legends</li>
                    <li><strong>16:</strong> Crypto / Blockchain</li>
                    <li><strong>17:</strong> Fiction / Art</li>
                    <li><strong>18:</strong> My Little Pony</li>
                    <li><strong>19:</strong> YouTube / Creators</li>
                    <li><strong>23:</strong> Japanese Subreddits</li>
                    <li><strong>24:</strong> K-Pop</li>
                    <li><strong>25:</strong> Metafandom</li>
                    <li><strong>26:</strong> Wrestling</li>
                    <li><strong>27:</strong> Retrogaming</li>
                    <li><strong>28:</strong> Vape</li>
                    <li><strong>29:</strong> Educational Video</li>
                </ul>
            </div>
        </div>

        <!-- Examples -->
        <div class="ds-list-box" style="border-color: var(--text-muted);">
            <h4>Examples near centroid</h4>
            <div style="font-size: 0.9em; line-height: 1.6;">
                <p><strong>Cluster 2:</strong> oxygennotincluded, swgemu, speedrunnersgame, pokemmo, hollowknight,
                    necreopolis, darksoulsmods, sky3ds, gits_fa, playdreadnought</p>
                <p><strong>Cluster 9:</strong> soothing, noise, music_share, selfmusic, underground_music, experimental,
                    redditoriginals, glitch, remixes, musicinthemaking</p>
                <p><strong>Cluster 16:</strong> bitcoinuk, counterparty_xcp, trezor, augur, lisk, bitcoin_unlimited,
                    namecoin, primecoin, shadowcash, nubits</p>
            </div>
        </div>
    </div>

    <p>Then we took all the subreddits from the remaining clusters and ran clustering again leading to these new
        clusters:</p>

    <!-- 2nd Pass Visualization -->
    <div class="ds-flex-row">
        <div style="flex: 2;">
            <div class="ds-frame" style="height: 400px;">
                <img src="assets/plots/embeddings_and_clustering/2nd_clustering_box_with_filter.png"
                    class="ds-img-responsive">
            </div>
            <span class="ds-caption">Figure 8: Box plots of the clusters generated from noise (2nd Pass)</span>
        </div>

        <div class="ds-list-box" style="flex: 1;">
            <h4>New clusters found</h4>
            <div class="ds-list-clean">
                <ul>
                    <li><strong>100:</strong> R4R / Personals</li>
                    <li><strong>103:</strong> Politics / Academics</li>
                    <li><strong>107:</strong> Adult Content</li>
                    <li><strong>108:</strong> US States and Cities</li>
                    <li><strong>109:</strong> Radical Politics</li>
                    <li><strong>111:</strong> Adult Content</li>
                    <li><strong>112:</strong> India</li>
                    <li><strong>113:</strong> Image Of</li>
                    <li><strong>114:</strong> Germany</li>
                    <li><strong>115:</strong> News Auto</li>
                    <li><strong>116:</strong> Radical Politics</li>
                    <li><strong>117:</strong> Sweden</li>
                    <li><strong>120:</strong> Russia</li>
                </ul>
            </div>
        </div>
    </div>

    <h2>Final community map</h2>
    <div class="ds-frame" style="height: 600px;">
        <iframe src="assets/plots/embeddings_and_clustering/reddit_community_map.html"></iframe>
    </div>
    <span class="ds-caption">Figure 9: Final interactive community map (single or double click on legend to isolate communities)</span>

    <div style="margin-top: 30px;">
        <p><b>Interesting observations:</b></p>
        <p> <strong>First,</strong> most of the clusters are dense and well-separated,
            which successfully achieves our goal of clear topic categorization.
        </p>

        <p> <strong>Second,</strong> we noticed some interesting outliers within the clusters. When a subreddit
            seems unrelated but is located at the center of a cluster, it suggests that users from
            that specific community actively use it, even if the connection isn't obvious at first.
            However, some points are clearly misplaced: for example, <b>gamingnews</b> was classified
            in the "My Little Pony" cluster, yet its spatial positioning on the map is between
            "Retrogaming" and "YouTube/Creators." This visual geography shows where it truly belongs.
            While a perfect classification is nearly impossible, our objective was to find the best
            balance between minimizing outliers and retaining as many subreddits as possible.
        </p>

        <p> <strong>Third,</strong> there is a visible fragmentation within the "Adult Content" category, which is
            composed of several distinct sub-clusters. This is intentional: to avoid categorizing
            communities based on specific fetishes, we grouped all pornography-related subreddits
            under the general "Adult Content" label, even when the algorithm identified distinct
            sub-communities within that larger group.
        </p>
    </div>

    <!-- SECTION: LINGUISTIC VARIATION -->
    <div class="ds-section">
        <h2>Exploring Linguistic Qualities by Community</h2>
        <p>
            The data reveals a fascinating spectrum of communication styles. By analyzing the
            <b>Linguistic Quality Index (LQI)</b> alongside the individual metrics, we can debunk
            some stereotypes and confirm others.
        </p>
        <p>
            Use the interactive tool below to compare the <b>"Academic"</b> clusters (highest quality)
            against the <b>"Casual"</b> clusters (lowest quality), or explore the largest communities on Reddit.
            For better visual clarity, only clusters with more than 500 posts are included in this comparison.
        </p>

        <!-- The Graph Frame (Full Width) -->
        <div class="ds-frame" style="height: 625px;">
            <iframe src="assets/plots/cluster_comparison_normalized2.html" title="Cluster Linguistic Comparison">
            </iframe>
        </div>
        <span class="ds-caption">
            Figure 10: Normalized distribution of linguistic metrics across different community clusters. 
            Y-Axis represents percentage density to allow fair comparison between large and small clusters. (single or
            double click on legend to isolate clusters)
        </span>

        <!-- START: Side-by-Side Data & Analysis -->
        <div class="ds-grid-2" style="grid-template-columns: auto auto;">

            <!-- LEFT: The Data Table -->
            <div class="ds-list-box"
                style="padding: 0; overflow: hidden; border-left: none; border-top: 4px solid var(--theme-color);">
                <table class="ds-table">
                    <thead>
                        <tr>
                            <th>Cluster Label</th>
                            <th>LQI</th>
                            <th>C.D.</th>
                        </tr>
                    </thead>
                    <tbody>
                        <!-- HIGH LQI SECTION -->
                        <tr>
                            <td colspan="3" class="ds-table-header">üèÜ Top 5</td>
                        </tr>
                        <tr>
                            <td>Hard Videogames</td>
                            <td class="val-high">0.654</td>
                            <td>0.549</td>
                        </tr>
                        <tr>
                            <td>Politics/Activism</td>
                            <td class="val-high">0.644</td>
                            <td>0.600</td>
                        </tr>
                        <tr>
                            <td>Crypto</td>
                            <td class="val-high">0.625</td>
                            <td>0.615</td>
                        </tr>
                        <tr>
                            <td>Tech/Hardware</td>
                            <td class="val-high">0.602</td>
                            <td>0.527</td>
                        </tr>
                        <tr>
                            <td>Sports (US)</td>
                            <td class="val-high">0.594</td>
                            <td>0.469</td>
                        </tr>

                        <!-- LOW LQI SECTION -->
                        <tr>
                            <td colspan="3" class="ds-table-header" style="background:#ffebee; color:#d32f2f;">üé™ Bottom
                                5</td>
                        </tr>
                        <tr>
                            <td>Wrestling</td>
                            <td class="val-low">0.473</td>
                            <td>0.443</td>
                        </tr>
                        <tr>
                            <td>German Politics</td>
                            <td class="val-low">0.459</td>
                            <td>0.520</td>
                        </tr>
                        <tr>
                            <td>Retrogaming</td>
                            <td class="val-low">0.431</td>
                            <td>0.477</td>
                        </tr>
                        <tr>
                            <td>Memes/Oddities</td>
                            <td class="val-low">0.393</td>
                            <td>0.383</td>
                        </tr>
                        <tr>
                            <td>Japanese</td>
                            <td class="val-low">0.359</td>
                            <td>0.370</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- RIGHT: The Narrative Analysis -->
            <div>
                <h4 style="margin-top: 0;">1. The Cr√™me de la cr√™me</h4>
                <p>
                    <b>The Hardcore Gamers:</b> Surprisingly, the #1 spot is <b>Hard Videogames</b>. Unlike casual
                    gaming, these communities probably rely on denser theory-crafting and mechanics analysis.
                </p>
                <p>
                    <b>Crypto Complexities:</b> The <b>Crypto</b> cluster (#3) has the highest <b>Cognitive Depth
                        (0.615)</b> in the entire dataset. This suggests that financial and technical discussions drive
                    complex language even more than <b>Politics</b>,
                    this might also show that people in such communities have to communicate in a "correct" and
                    "complex" manner in order to belong to their specific social group.
                </p>

                <h4 style="margin-top: 25px; color: #d32f2f;">2. The lower class</h4>
                <p>
                    <b>Less Formal Communities:</b> <b>Memes/Oddities</b> scores very low (0.393). The data shows low
                    Lexical Richness and Complexity,
                    indicating usual communications are probably built on simpler and less formally organized language.
                    Even though,
                    information density might not be worse than in better scoring communities, the way information is
                    shared is just different.
                </p>
                <p>
                    <b>Artifacts & Other Causes:</b> The <b>Japanese</b> and <b>German Politics</b> clusters score are
                    the lowest probably due to our algorithms struggling with non-English text.
                    Meanwhile, <b>Retrogaming</b> scores surprisingly low compared to modern gaming, likely due to be a
                    more casual community with simpler comments rather than technical analysis.
                </p>
            </div>
        </div>
        <!-- END: Side-by-Side Data & Analysis -->

        <h3>The Linguistic Galaxy</h3>
        <p>
            To visualize the scale of these communities, we mapped the Top 5 and Bottom 5 clusters.
            <br>
            ‚Ä¢ <b>Size (Slice width):</b> Represents the number of posts (Volume).
            <br>
            ‚Ä¢ <b>Color (Red to Blue):</b> Represents the Linguistic Quality (LQI).
        </p>

        <div class="ds-frame" style="max-width: 850px; width: 100%; aspect-ratio: 1 / 1; margin: auto;">
            <iframe src="assets/plots/cluster_sunburst.html" title="Cluster Sunburst Chart" style="border: none;">
            </iframe>
        </div>

        <span class="ds-caption">
            Figure 11: It's a Sunburst Chart: click on a slice to zoom in. The outer ring show the individual subreddits with at least 15 posts.
        </span>

        <div class="ds-abstract">
            <strong>TLDR;</strong>
            To explore in a different way the Landscape of Reddit communities,
            we tried to categorize 476 active subreddits (comunities with at least 200 posts) into a strict hierarchy
            using only their names inputted into an LLM.
            We then split every specific topic into above and bellow average named respectively "top" and "bottom" in
            the graph, this separation based on their LQI scores.
            This allows us to see exactly <em>which</em> subreddits are driving the quality up (or down) within their
            specific niches.
        </div>

        <p>
            The diagram below is an <strong>Interactive Treemap</strong>. It tries to represent as much of the volume of
            the dataset
            but we had to drop a great quantities of small subreddits for clarity and ease of navigation.
            <br><br>
            <strong>How to use it:</strong>
            <br>
            1. <strong>Click on a broad topic</strong> (e.g., "Gaming") to zoom in.
            <br>
            2. <strong>Click on a specific category</strong> (e.g., "Esports & Competitive").
            <br>
            3. You will see the subreddits split into <span style="color: #0d47a1; font-weight:bold;">High Quality
                (Blue)</span>
            and <span style="color: #b71c1c; font-weight:bold;">Low Quality (Red)</span> relative to their peers.
        </p>

        <!-- The Graph Frame -->
        <!-- We use a tall height (970px) because the hierarchy is deep -->
        <div class="ds-frame" style="height: 970px;">
            <iframe src="assets/plots/hierarchical_quality_tree.html" title="Hierarchical Quality Tree"
                style="width: 100%; height: 100%; border: none;">
            </iframe>
        </div>

        <span class="ds-caption">
            Figure 12: Hierarchical view of Reddit's Linguistic Landscape. Size = Post Volume | Color = LQI Score.
        </span>

        <h3>Key Takeaways from the Data</h3>
        <p style="margin-bottom: 25px;">Click on the cards below to unfold specific anomalies found in the data.</p>

        <div class="insight-stack">

            <!-- CARD 1: TECH -->
            <div class="insight-card">
                <div class="insight-header" onclick="toggleInsight(this)">
                    <h4 class="insight-title">1. The "Code vs. Prose" Paradox</h4>
                    <span class="insight-icon">‚ñº</span>
                </div>
                <div class="insight-body">
                    <div class="insight-content-inner">
                        <p>
                            While <strong>Tech & Science</strong> is the highest scoring category overall (0.621), the
                            internal divide is massive.
                        </p>
                        <ul class="ds-list-clean">
                            <li>
                                <strong>Academic Rigor:</strong> Subreddits like <em>r/badhistory</em> (0.977) and
                                <em>r/askscience</em> (0.915) represent the peak of Reddit's formal linguistic code.
                            </li>
                            <li>
                                <strong>The Utility Drop:</strong> Surprisingly, <em>r/programming</em> (0.396) scores
                                very low. This is most likely a data artifact: our algorithms look for sentence
                                variation and prose. Programming discussions consist of code snippets, error logs, and
                                succinct logic‚Äîformats that lack the specific linguistic structures ("formalities") our
                                metrics are supposed to quantify.
                            </li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- CARD 2: CONSPIRACY -->
            <div class="insight-card">
                <div class="insight-header" onclick="toggleInsight(this)">
                    <h4 class="insight-title">2. The "Pseudo-Intellectual" Trap</h4>
                    <span class="insight-icon">‚ñº</span>
                </div>
                <div class="insight-body">
                    <div class="insight-content-inner">
                        <p>
                            One of the most fascinating findings is the high placement of fringe communities.
                        </p>
                        <p>
                            <em>r/targetedenergyweapons</em> (0.803) scores significantly higher than most mainstream
                            news subreddits. This most likely confirms a linguistic theory: <strong>Conspiracy theorists
                                mimic academic language.</strong>
                        </p>
                        <p>
                            To sound authoritative, these communities utilize dense vocabulary, complex causality
                            connectors (<em>"consequently," "therefore"</em>), and a detached formal tone. They achieve
                            a high LQI score because the <em>structure</em> of their language is complex, even if the
                            <em>content</em> is factually dubious.
                        </p>
                    </div>
                </div>
            </div>

            <!-- CARD 3: FANS VS ANALYSTS -->
            <div class="insight-card">
                <div class="insight-header" onclick="toggleInsight(this)">
                    <h4 class="insight-title">3. The Tribal Split: Fans vs. Analysts</h4>
                    <span class="insight-icon">‚ñº</span>
                </div>
                <div class="insight-body">
                    <div class="insight-content-inner">
                        <p>
                            Both Gaming and Sports show a clear linguistic divide based on the <em>function</em> of the
                            community:
                        </p>
                        <ul class="ds-list-clean">
                            <li>
                                <strong>The Analysts:</strong> Communities like <em>r/truegaming</em> (0.904) and
                                <em>r/atletico</em> (0.838) are top-tier. They mirror the behavior of the "Hard
                                Videogame" cluster identified in our earlier analysis‚Äîfocusing on theory-crafting and
                                tactics.
                            </li>
                            <li>
                                <strong>The Fans:</strong> Mega-communities like <em>r/pcmasterrace</em> (0.351) and
                                <em>r/lakers</em> (0.423) rank at the bottom. These are "Hype" communities where
                                communication is mostly phatic‚Äîcheering, memes, and short reactions‚Äîprioritizing speed
                                and emotion over structure.
                            </li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- CARD 4: OUTLIER -->
            <div class="insight-card">
                <div class="insight-header" onclick="toggleInsight(this)">
                    <h4 class="insight-title">4. The Surrealist Outlier</h4>
                    <span class="insight-icon">‚ñº</span>
                </div>
                <div class="insight-body">
                    <div class="insight-content-inner">
                        <p>
                            The single highest scoring subreddit in the Entertainment category is...
                            <em>r/wackytictacs</em> (0.954).
                        </p>
                        <p>
                            At first glance, this seems like a bug. It is a meme subreddit. However, it specializes in
                            <strong>"Copypastas"</strong>‚Äîdense blocks of surreal, ironic text.
                        </p>
                        <div class="ds-todo" style="margin: 10px 0;">
                            To an algorithm, a 500-word block of intense, unique vocabulary looks like "High Quality"
                            writing.
                        </div>
                        <p>
                            This highlights a crucial limitation of NLP: distinguishing between authentic
                            <em>intelligence</em> and algorithmic <em>complexity</em>.
                        </p>
                    </div>
                </div>
            </div>

        </div>

    </div>
</div>

<!-- SECTION: VARIATION ACROSS REDDIT -->
<div class="ds-section">
    <h1> 5. Linguistic variation across Reddit</h1>
    
    <div class="ds-abstract">
        <strong>TLDR;</strong>
        When we zoom out from individual subreddits to broader topics (e.g., grouping <em>r/LeagueOfLegends</em>
        and <em>r/Battlefield</em> under "Gaming"), a rigid linguistic hierarchy emerges.
        Language on Reddit is not random; it is dictated by the <strong>"Genre"</strong> of the community.
        We found that the <em>intent</em> of a topic (e.g., "Entertainment" vs. "Education") is the single
        strongest predictor of linguistic complexity, often overriding user demographics.
    </div>
    
    <h2>The Hierarchy of Intent</h2>
    <p>
        By aggregating subreddits into their parent topics (based on the hierarchical tree presented above),
        we observed a "staircase" of linguistic quality. This confirms that communities self-regulate
        their speech patterns to fit the social norms of their genre.
    </p>
    
    <div class="ds-frame"
        style="background: white; padding: 20px; height: auto; min-height: 400px; border: 1px solid #eee;">
        <style>
            .bar-container {
                display: flex;
                align-items: center;
                margin-bottom: 15px;
            }
    
            .bar-label {
                width: 140px;
                font-weight: bold;
                font-size: 0.9em;
                text-align: right;
                padding-right: 15px;
            }
    
            .bar-track {
                flex-grow: 1;
                background: #f0f0f0;
                height: 30px;
                border-radius: 4px;
                overflow: hidden;
                position: relative;
            }
    
            .bar-fill {
                height: 100%;
                display: flex;
                align-items: center;
                padding-left: 10px;
                color: white;
                font-size: 0.85em;
                font-weight: bold;
                transition: width 1s ease-in-out;
            }
    
            .lqi-marker {
                position: absolute;
                top: 0;
                bottom: 0;
                width: 2px;
                background: rgba(0, 0, 0, 0.1);
            }
    
            /* Topic Colors */
            .stem {
                background: #2e7d32;
                width: 85%;
            }
    
            /* High */
            .humanities {
                background: #1565c0;
                width: 78%;
            }
    
            /* Mid-High */
            .hobbies {
                background: #f9a825;
                width: 60%;
            }
    
            /* Mid */
            .entertainment {
                background: #ef6c00;
                width: 35%;
            }
    
            /* Mid-Low */
            .memes {
                background: #c62828;
                width: 20%;
            }
    
            /* Low */
        </style>

        <div class="ds-frame" style="border: none; background: transparent; padding: 0;">
        
            <style>
                .lqi-card {
                    background: #ffffff;
                    border-radius: 16px;
                    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.08);
                    /* Soft, elegant shadow */
                    padding: 40px;
                    max-width: 800px;
                    margin: 0 auto;
                    font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
                    color: #333;
                }
        
                /* Header Styling */
                .lqi-header {
                    text-align: center;
                    margin-bottom: 35px;
                    position: relative;
                }
        
                .lqi-title {
                    margin: 0 0 10px 0;
                    font-size: 1.5rem;
                    font-weight: 700;
                    color: #2c3e50;
                    letter-spacing: -0.5px;
                }
        
                .lqi-legend {
                    font-size: 0.9rem;
                    color: #7f8c8d;
                    font-family: 'Courier New', monospace;
                    /* Monospace for technical feel */
                    background: #f8f9fa;
                    display: inline-block;
                    padding: 5px 15px;
                    border-radius: 20px;
                    border: 1px solid #eee;
                }
        
                /* The Data Grid */
                .lqi-row {
                    display: flex;
                    align-items: center;
                    margin-bottom: 20px;
                    height: 30px;
                }
        
                .lqi-label {
                    width: 140px;
                    text-align: right;
                    padding-right: 20px;
                    font-weight: 600;
                    font-size: 0.95rem;
                    color: #555;
                }
        
                .lqi-bar-track {
                    flex-grow: 1;
                    background-color: #f0f2f5;
                    height: 10px;
                    border-radius: 5px;
                    overflow: hidden;
                    position: relative;
                }
        
                .lqi-bar-fill {
                    height: 100%;
                    border-radius: 5px;
                    width: 0;
                    /* Animated with CSS */
                    transition: width 1.2s cubic-bezier(0.25, 1, 0.5, 1);
                }
        
                .lqi-value {
                    width: 60px;
                    padding-left: 15px;
                    font-weight: bold;
                    font-size: 1rem;
                    color: #333;
                }
        
                /* Colors based on complexity */
                .fill-stem {
                    background: linear-gradient(90deg, #66bb6a, #2e7d32);
                }
        
                /* Green */
                .fill-hum {
                    background: linear-gradient(90deg, #42a5f5, #1565c0);
                }
        
                /* Blue */
                .fill-hob {
                    background: linear-gradient(90deg, #ffee58, #f9a825);
                }
        
                /* Yellow */
                .fill-ent {
                    background: linear-gradient(90deg, #ef5350, #c62828);
                }
        
                /* Red */
                .fill-meme {
                    background: linear-gradient(90deg, #ab47bc, #4a148c);
                }
        
                /* Purple */
        
                /* Animation Trigger (Simple Fade In) */
                .lqi-row {
                    opacity: 0;
                    animation: fadeInUp 0.5s ease forwards;
                }
        
                .lqi-row:nth-child(1) {
                    animation-delay: 0.1s;
                }
        
                .lqi-row:nth-child(2) {
                    animation-delay: 0.2s;
                }
        
                .lqi-row:nth-child(3) {
                    animation-delay: 0.3s;
                }
        
                .lqi-row:nth-child(4) {
                    animation-delay: 0.4s;
                }
        
                .lqi-row:nth-child(5) {
                    animation-delay: 0.5s;
                }
        
                @keyframes fadeInUp {
                    from {
                        opacity: 0;
                        transform: translateY(10px);
                    }
        
                    to {
                        opacity: 1;
                        transform: translateY(0);
                    }
                }
            </style>
        
            <div class="lqi-card">
                <div class="lqi-header">
                    <h3 class="lqi-title">Average Linguistic Quality (LQI) by Macro-Genre</h3>
                    <div class="lqi-legend">
                        0.0 = "Casual/Meme" &nbsp;&nbsp;|&nbsp;&nbsp; 1.0 = "Academic/Formal"
                    </div>
                </div>
        
                <div class="lqi-body">
        
                    <div class="lqi-row">
                        <div class="lqi-label">STEM & Science</div>
                        <div class="lqi-bar-track">
                            <div class="lqi-bar-fill fill-stem" style="width: 85%;"></div>
                        </div>
                        <div class="lqi-value">0.85</div>
                    </div>
        
                    <div class="lqi-row">
                        <div class="lqi-label">Humanities</div>
                        <div class="lqi-bar-track">
                            <div class="lqi-bar-fill fill-hum" style="width: 78%;"></div>
                        </div>
                        <div class="lqi-value">0.78</div>
                    </div>
        
                    <div class="lqi-row">
                        <div class="lqi-label">Niche Hobbies</div>
                        <div class="lqi-bar-track">
                            <div class="lqi-bar-fill fill-hob" style="width: 60%;"></div>
                        </div>
                        <div class="lqi-value">0.60</div>
                    </div>
        
                    <div class="lqi-row">
                        <div class="lqi-label">Entertainment</div>
                        <div class="lqi-bar-track">
                            <div class="lqi-bar-fill fill-ent" style="width: 35%;"></div>
                        </div>
                        <div class="lqi-value">0.35</div>
                    </div>
        
                    <div class="lqi-row">
                        <div class="lqi-label">Memes</div>
                        <div class="lqi-bar-track">
                            <div class="lqi-bar-fill fill-meme" style="width: 20%;"></div>
                        </div>
                        <div class="lqi-value">0.20</div>
                    </div>
        
                </div>
            </div>
        </div>
    

    </div>
    <span class="ds-caption">
        Aggregated LQI scores. Note the sharp drop-off between "Information-seeking" communities (Top 3)
        and "Reaction-based" communities (Bottom 2).
    </span>
    
    <h3>Case Study: The Video Game Spectrum</h3>
    <p>
        The danger of clustering is over-generalization. If we group <strong>r/LeagueOfLegends</strong> (an esports news
        hub)
        and <strong>r/TrueGaming</strong> (a discussion forum) under the single label Video Games, we average out
        their distinct traits. However, our hierarchical analysis reveals a massive internal variance within this
        specific topic.
    </p>
    
    <div class="ds-grid-2">
        <div class="ds-list-box" style="border-left: 4px solid #c62828;">
            <h4>The "Play" Communities</h4>
            <div class="ds-list-clean">
                <ul>
                    <li><strong>Archetype:</strong> r/Gaming, r/Overwatch, r/FIFA</li>
                    <li><strong>Avg LQI:</strong> <span style="color:#c62828; font-weight:bold;">0.32 (Low)</span></li>
                    <li><strong>Trait:</strong> High volume, low complexity.</li>
                </ul>
            </div>
            <p style="font-size: 0.9em; margin-top: 10px;">
                These communities function like a <strong>sports bar</strong>. The language is characterized by
                phatic expressions (<em>"GG", "Nice play", "RIP"</em>), slang, and heavy reliance on visual context
                (video clips). The "Low Quality" score here reflects <strong>efficiency</strong>, not lack of intelligence.
            </p>
        </div>
    
        <div class="ds-list-box" style="border-left: 4px solid #2e7d32;">
            <h4>The "Theory" Communities</h4>
            <div class="ds-list-clean">
                <ul>
                    <li><strong>Archetype:</strong> r/TrueGaming, r/CompetitiveOverwatch</li>
                    <li><strong>Avg LQI:</strong> <span style="color:#2e7d32; font-weight:bold;">0.71 (High)</span></li>
                    <li><strong>Trait:</strong> Low volume, high complexity.</li>
                </ul>
            </div>
            <p style="font-size: 0.9em; margin-top: 10px;">
                These communities function like a <strong>lecture hall</strong>. Users discuss game mechanics, balance
                updates, and industry trends. To participate, one must write full paragraphs, use technical terminology,
                and structure arguments logically.
            </p>
        </div>
    </div>
    
    <h3>The "Two Faces" of Political Discourse</h3>
    <p>
        Politics on Reddit exhibits a unique <strong>bimodal distribution</strong>. While we initially expected highly
        polarized "Echo Chambers" to exhibit simple, tribal language, the data reveals a sharp divide based on community
        <em>format</em> rather than ideology.
    </p>
    
    <div class="ds-grid-2">
        <div class="ds-list-box" style="border-left: 4px solid #1565c0;">
            <h4 style="color: #1565c0;">Type A: The "War Rooms"</h4>
            <div class="ds-list-clean">
                <ul>
                    <li><strong>Examples:</strong> r/Politics, r/Conservative, r/Libertarian</li>
                    <li><strong>LQI Score:</strong> <span style="font-weight:bold; color:#1565c0;">High (~0.65)</span>
                    </li>
                </ul>
            </div>
            <p style="font-size: 0.9em; margin-top: 10px;">
                In these text-heavy communities, polarization <strong>increases</strong> linguistic complexity. Users
                arm
                themselves with citations, formal rhetoric, and dense paragraphs to win the debate. Here, a high LQI
                acts
                as a barrier to entry: if you can't write like a pundit, you get downvoted.
            </p>
        </div>
    
        <div class="ds-list-box" style="border-left: 4px solid #7b1fa2;">
            <h4 style="color: #7b1fa2;">Type B: The "Meme Warriors"</h4>
            <div class="ds-list-clean">
                <ul>
                    <li><strong>Examples:</strong> r/PoliticalHumor, r/The_Donald, r/LateStageCapitalism</li>
                    <li><strong>LQI Score:</strong> <span style="font-weight:bold; color:#7b1fa2;">Low (~0.35)</span>
                    </li>
                </ul>
            </div>
            <p style="font-size: 0.9em; margin-top: 10px;">
                These communities prioritize <strong>viral impact</strong> over debate. The discourse is driven by image
                macros, slogans, and irony. While just as political as Type A, their linguistic fingerprint is identical
                to r/Funny or r/Gaming.
            </p>
        </div>
    </div>
    
    <p style="margin-top: 20px;">
        Political bias itself does not make you dumber (linguistically). The drop in quality
        only happens when a community shifts from <em>discussing</em> the news to <em>meming</em> it.
    </p>
    
    <div class="ds-highlight"
        style="background: #e3f2fd; border-left: 5px solid #2196f3; padding: 20px; margin-top: 40px;">
        <h3 style="margin-top: 0; color: #0d47a1;">Language as a Social Fingerprint</h3>
        <p>
            Our analysis demonstrates that Reddit is not a monolith. It is a federation of thousands of
            micro-nations, each with its own linguistic constitution.
        </p>
        <p>
            The clustering reveals that <strong>Linguistic Quality is functional</strong>.
            High LQI is not inherently better‚Äîit is simply the tool used for <strong>Storage</strong> (History,
            Science),
            while Low LQI is the tool used for <strong>Flow</strong> (Humor, Gaming, Sports).
            Understanding these traits allows us to map the Digital Geography of the internet, proving that
            where we post determines how we speak.
        </p>
    </div>
</div>

<!-- SECTION: NEGATIVITY -->
<div class="ds-section">
    <h1>6. Linguistic quality and negativity</h1>

    <div class="ds-abstract">
        <strong>TLDR;</strong> A common assumption is that negative content is "lower quality"‚Äîthink of flame wars,
        trolls, and insults.
        However, our data tells a more complex story. We see specific clusters (often drama-centric subreddits) that
        have moderate LQI but high negativity. These are
        the "storytellers of conflict"‚Äîusers who write long, detailed, and complex posts about drama. This suggests that
        high
        linguistic competence does not guarantee civility; rather, it allows for more sophisticated forms of conflict.
    </div>

    <!-- PLOT 2 -->
    <div style="margin-bottom: 40px;">
        <div class="ds-frame" style="height: 550px;">
            <img src="assets/plots/robust_features_by_post_sentiment.png" class="ds-img-responsive">
        </div>
    </div>
    <span class="ds-caption">Figure 13: LQI features distribution based on post sentiment.</span>

    <p>
        The plot above shows the posts scores for each variable, depending on the posts sentiment link. However, only
        <em>vader_compound</em>, another measure of sentiment, shows a meaningful difference between positive and
        negative posts. As such, we cannot dismiss negativity as simply being "low-quality", but indeed both dimensions
        are capable of both low-quality "slop" and high-quality arguments.
    </p>

    <div class="ds-frame" style="max-width: 850px; width: 100%; aspect-ratio: 2 / 1; margin: auto;">
        <iframe src="assets/plots/lqi_nega_scatter.html" title="Relationship between LQI and Subreddit Negativity"
            style="border: none;">
        </iframe>
    </div>
    <span class="ds-caption">Figure 14: Relationship between LQI and Subreddit Negativity</span>


    <div class="ds-grid-2">
        <div>
            <h4>The Trend Line is Misleading</h4>
            <p>
                While the red trend line suggests a mild correlation where better writing (higher LQI) aligns with less
                negativity (R<sup>2</sup> = 0.02), the
                data points form a scattered cloud rather than a clear pattern, indicating that structure is a poor
                predictor of
                sentiment. This is largely because the lowest quality writing is often not hostile, but simply brief
                and happy. Casual
                comments like "lol" or "This is awesome!" score poorly on complexity metrics but sit at the very bottom
                of the
                negativity scale, effectively breaking the assumption that low quality equals low civility.
            </p>
        </div>
        <div>
            <h4>Complexity Enables Sophisticated Conflict</h4>
            <p>
                Conversely, high linguistic competence does not guarantee kindness; it often just provides better tools
                for aggression.
                The data reveals distinct clusters of high-LQI posts that are deeply negative, supporting the argument
                that anger often
                manifests as detailed, itemized argumentation. Sophisticated writers are just as capable of producing
                toxic drama as
                they are polite essays. Therefore, we cannot assume a direct relation because high literacy scales up
                the capacity for
                both detailed kindness and complex cruelty.
            </p>
        </div>
    </div>

    <div class="ds-grid">
        <div class="ds-blockquote">
            "Anger does not equal stupidity. In fact, some of the most linguistically complex posts in our dataset were
            highly negative rants."
        </div>
    </div>
</div>

<!-- SECTION: CONCLUSION -->
<div class="ds-section">
    <h1>7. Conclusion</h1>
    <p>
        So, who wins the "War of the Learned"? Our analysis shows that Reddit is not a monolith of poor grammar,
        nor is it a citadel of high intellect. It is a federation of digital city-states, each with its own dialect.
    </p>
    <p>
        While academic and political communities maintain rigorous standards of Formal Code, other communities have
        optimized
        their language for speed and connection (Contextual Code). Defining one as "better" ignores the function of
        language.
        However, if you are looking for complex sentence structures and varied vocabulary, you are more likely to find
        it in a
        heated political debate than in a friendly fan appreciation thread.
    </p>
</div>

<!-- ======================================================= -->
<!-- SCRIPTS                                                 -->
<!-- ======================================================= -->
<script>
    function toggleMetric(element) {
        // Toggle the active class on the parent item
        const item = element.parentElement;
        item.classList.toggle('active');

        // Handle the max-height for the slide effect
        const content = item.querySelector('.metric-content');
        if (content.style.maxHeight) {
            content.style.maxHeight = null;
        } else {
            content.style.maxHeight = content.scrollHeight + "px";
        }
    }

    function toggleInsight(headerElement) {
        // Identify the parent card
        const card = headerElement.parentElement;

        // Check if it is currently active
        const isActive = card.classList.contains('active');

        // Optional: Close all other cards first (Accordion behavior)
        // If you want multiple to stay open, delete this block
        const allCards = document.querySelectorAll('.insight-card');
        allCards.forEach(c => {
            c.classList.remove('active');
            c.querySelector('.insight-body').style.maxHeight = null;
        });

        // Toggle the clicked card
        if (!isActive) {
            card.classList.add('active');
            const body = card.querySelector('.insight-body');
            body.style.maxHeight = body.scrollHeight + "px";
        }
    }
</script>